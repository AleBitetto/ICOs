{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75e337e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "import datetime\n",
    "from unidecode import unidecode\n",
    "import pickle\n",
    "import pdfkit\n",
    "import joblib\n",
    "import func_timeout\n",
    "# from lingua import Language, LanguageDetectorBuilder\n",
    "from utils import get_chromedriver, download_from_drive_dropbox, pdf_to_text\n",
    "\n",
    "TESSERACT_PATH=r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'   # used for tesseract OCR. See pdf_to_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b85a0860",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMEDRIVER_PATH = r\"C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\ICOs\\WebDriver\\chromedriver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f55d8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set folders\n",
    "WHITEPAPER_FOLDER = '.\\\\Whitepaper'\n",
    "ORIGINAL_FOLDER = '.\\\\Whitepaper\\\\Original'\n",
    "CONVERTED_FOLDER = '.\\\\Whitepaper\\\\Converted_to_txt'\n",
    "RECOVERED_FOLDER = '.\\\\Whitepaper\\\\Recovered'\n",
    "CHECKPOINT_FOLDER = '.\\\\Checkpoints'\n",
    "RESULTS_FOLDER = '.\\\\Results'\n",
    "\n",
    "if not os.path.exists(WHITEPAPER_FOLDER):\n",
    "    os.makedirs(WHITEPAPER_FOLDER)\n",
    "if not os.path.exists(ORIGINAL_FOLDER):\n",
    "    os.makedirs(ORIGINAL_FOLDER)\n",
    "if not os.path.exists(CONVERTED_FOLDER):\n",
    "    os.makedirs(CONVERTED_FOLDER)\n",
    "if not os.path.exists(RECOVERED_FOLDER):\n",
    "    os.makedirs(RECOVERED_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a34c6b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_DOMAIN = \".ai\"      # replace the old domani \".com\", changed after first run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2eb4d090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>WhitepaperUrl</th>\n",
       "      <th>Status</th>\n",
       "      <th>Error</th>\n",
       "      <th>Path_Original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://icomarks.ai/ico/tokelite</td>\n",
       "      <td>https://www.docdroid.net/xvdu93N/tokelite-whit...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://icomarks.ai/ico/ucbi-banking</td>\n",
       "      <td>https://ucbibanking.com/UCBI_Whitepaper_EN.pdf</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://icomarks.ai/ico/capitual</td>\n",
       "      <td>https://capitual.io/whitepaper.pdf</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://icomarks.ai/ico/btccredit</td>\n",
       "      <td>https://btccredit.io/pdf/BTCCredit_Whitepaper_...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://icomarks.ai/ico/ledgerium</td>\n",
       "      <td>https://whitepaper.ledgerium.io/</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7741</th>\n",
       "      <td>https://icomarks.ai/ico/deepcloud</td>\n",
       "      <td>https://www.dropbox.com/s/st6ldsd5shfdz3y/Deep...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7742</th>\n",
       "      <td>https://icomarks.ai/ico/tap4-menu</td>\n",
       "      <td>http://tap4.menu/wp-content/uploads/2019/02/Wh...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7743</th>\n",
       "      <td>https://icomarks.ai/ico/ultrablock</td>\n",
       "      <td>https://ultrablock.io/pdf/whitepaper.pdf</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7744</th>\n",
       "      <td>https://icomarks.ai/ico/avinoc</td>\n",
       "      <td>https://static.avinoc.cloud/downloads/AVINOC_W...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7745</th>\n",
       "      <td>https://icomarks.ai/ico/payera</td>\n",
       "      <td>https://payera.io/WHITEPAPER.pdf</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7746 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       url  \\\n",
       "0         https://icomarks.ai/ico/tokelite   \n",
       "1     https://icomarks.ai/ico/ucbi-banking   \n",
       "2         https://icomarks.ai/ico/capitual   \n",
       "3        https://icomarks.ai/ico/btccredit   \n",
       "4        https://icomarks.ai/ico/ledgerium   \n",
       "...                                    ...   \n",
       "7741     https://icomarks.ai/ico/deepcloud   \n",
       "7742     https://icomarks.ai/ico/tap4-menu   \n",
       "7743    https://icomarks.ai/ico/ultrablock   \n",
       "7744        https://icomarks.ai/ico/avinoc   \n",
       "7745        https://icomarks.ai/ico/payera   \n",
       "\n",
       "                                          WhitepaperUrl Status Error  \\\n",
       "0     https://www.docdroid.net/xvdu93N/tokelite-whit...                \n",
       "1        https://ucbibanking.com/UCBI_Whitepaper_EN.pdf                \n",
       "2                    https://capitual.io/whitepaper.pdf                \n",
       "3     https://btccredit.io/pdf/BTCCredit_Whitepaper_...                \n",
       "4                      https://whitepaper.ledgerium.io/                \n",
       "...                                                 ...    ...   ...   \n",
       "7741  https://www.dropbox.com/s/st6ldsd5shfdz3y/Deep...                \n",
       "7742  http://tap4.menu/wp-content/uploads/2019/02/Wh...                \n",
       "7743           https://ultrablock.io/pdf/whitepaper.pdf                \n",
       "7744  https://static.avinoc.cloud/downloads/AVINOC_W...                \n",
       "7745                   https://payera.io/WHITEPAPER.pdf                \n",
       "\n",
       "     Path_Original  \n",
       "0                   \n",
       "1                   \n",
       "2                   \n",
       "3                   \n",
       "4                   \n",
       "...            ...  \n",
       "7741                \n",
       "7742                \n",
       "7743                \n",
       "7744                \n",
       "7745                \n",
       "\n",
       "[7746 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load whitepaper url\n",
    "download_df = pd.read_csv(os.path.join(RESULTS_FOLDER, '01g_ICOmarks_ico_list_scraped_formatted.csv'), sep = \";\")\n",
    "\n",
    "download_df = download_df[['url', 'WhitepaperUrl']].dropna().reset_index(drop = True)\n",
    "download_df['Status'] = \"\"\n",
    "download_df['Error'] = \"\"\n",
    "download_df['Path_Original'] = \"\"\n",
    "\n",
    "download_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67055fbe",
   "metadata": {},
   "source": [
    "## Download pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5059bf5c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 7513 / 7513  - Total OK: 3046\n",
      "Total elapsed time: 11:06:25\n"
     ]
    }
   ],
   "source": [
    "HEADERS = {\"User-Agent\": \"Chrome/51.0.2704.103\"}\n",
    "URL_ROOT='https://icomarks.com/ico/'    # will be removed from url to create pdf name\n",
    "RELOAD_PDF=True\n",
    "\n",
    "start = timer()\n",
    "OK_count = 0\n",
    "for index, row in download_df.iterrows():\n",
    "    \n",
    "    url = row['WhitepaperUrl']\n",
    "    file_name = os.path.join(ORIGINAL_FOLDER, row['url'].replace(URL_ROOT.replace(\".com\", NEW_DOMAIN), '').replace('|', '') + '.pdf')\n",
    "\n",
    "    print('Downloading ' + str(index + 1) + ' / ' + str(len(download_df)) + '  - Total OK: ' + str(OK_count), end = '\\r')\n",
    "    \n",
    "    if not RELOAD_PDF or not os.path.exists(file_name):\n",
    "    \n",
    "        try:\n",
    "            # connect\n",
    "            response = requests.get(url, headers = HEADERS)\n",
    "\n",
    "            # check response and save pdf\n",
    "            if response.status_code == 200:\n",
    "                with open(file_name, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "                download_df.loc[index, 'Status'] = 'OK'\n",
    "                download_df.loc[index, 'Path_Original'] = os.path.join(os.getcwd(), file_name)\n",
    "                OK_count += 1\n",
    "            else:\n",
    "                download_df.loc[index, 'Status'] = response.status_code\n",
    "\n",
    "        except Exception as e:\n",
    "            download_df.loc[index, 'Status'] = 'ERROR'\n",
    "            download_df.loc[index, 'Error'] = e\n",
    "    \n",
    "    else:\n",
    "        download_df.loc[index, 'Status'] = 'OK'\n",
    "        download_df.loc[index, 'Path_Original'] = os.path.join(os.getcwd(), file_name)\n",
    "        OK_count += 1\n",
    "    \n",
    "    # save checkpoint\n",
    "    download_df.to_csv(os.path.join(CHECKPOINT_FOLDER, 'whitepaper_download.csv'), index=False, sep=';')\n",
    "            \n",
    "print('\\nTotal elapsed time:', str(datetime.timedelta(seconds=round(timer()-start))))\n",
    "\n",
    "# save results\n",
    "download_df.to_csv(os.path.join(RESULTS_FOLDER ,'00a_whitepaper_download_original.csv'), index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97523b13",
   "metadata": {},
   "source": [
    "## Check downloaded files and convert to txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6ce1373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK       3048\n",
      "ERROR    2710\n",
      "404      1227\n",
      "403       165\n",
      "410        91\n",
      "522        86\n",
      "520        73\n",
      "500        23\n",
      "521        17\n",
      "530        12\n",
      "502        11\n",
      "503        10\n",
      "504         8\n",
      "526         7\n",
      "523         6\n",
      "301         5\n",
      "525         4\n",
      "401         2\n",
      "524         2\n",
      "406         2\n",
      "400         2\n",
      "402         1\n",
      "423         1\n",
      "Name: Status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "download_df = pd.read_csv(os.path.join(RESULTS_FOLDER, '00a_whitepaper_download_original.csv'), sep = \";\")\n",
    "print(download_df['Status'].value_counts())\n",
    "\n",
    "final_df = download_df.copy()\n",
    "final_df = final_df[final_df['Status'] == 'OK'].reset_index(drop = True)\n",
    "final_df['Path_Recovered'] = \"\"\n",
    "final_df['Path_txt'] = \"\"\n",
    "final_df['Status_txt'] = \"\"\n",
    "final_df['Length_txt'] = 0\n",
    "final_df['Length_txt_clean'] = 0\n",
    "final_df['Content_txt'] = \"\"\n",
    "final_df['Metadata'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "016a2874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing 3048 / 3048\n",
      "Total elapsed time: 1:53:39\n"
     ]
    }
   ],
   "source": [
    "# Parse pdf and convert to txt\n",
    "URL_ROOT='https://icomarks.com/ico/'    # will be removed from url to create pdf name\n",
    "RELOAD_PKL=True\n",
    "\n",
    "tot_time=0\n",
    "for index, row in final_df.iterrows():\n",
    "    \n",
    "    file_path = row['Path_Original']\n",
    "    file_path_txt = os.path.join(os.getcwd(), CONVERTED_FOLDER, row['url'].replace(URL_ROOT.replace(\".com\", NEW_DOMAIN), '').replace('|', '') + '.txt')\n",
    "    file_path_pkl = os.path.join(os.getcwd(), CONVERTED_FOLDER, row['url'].replace(URL_ROOT.replace(\".com\", NEW_DOMAIN), '').replace('|', '') + '.pkl')\n",
    "    \n",
    "    print('Parsing ' + str(index + 1) + ' / ' + str(len(final_df)), end = '\\r')\n",
    "    \n",
    "    if not RELOAD_PKL or not os.path.exists(file_path_pkl):\n",
    "        \n",
    "        # pdf to txt\n",
    "        start = timer()\n",
    "        txt, meta, parsed_pdf = pdf_to_text(file_path=file_path, tesseract_path=TESSERACT_PATH, lang='eng')\n",
    "        status=parsed_pdf['status']\n",
    "        eval_time=datetime.timedelta(seconds=round(timer()-start)).total_seconds()\n",
    "        \n",
    "        # save .pkl\n",
    "        joblib.dump({'txt': txt, 'meta': meta, 'status': status, 'eval_time': eval_time}, file_path_pkl)\n",
    "    \n",
    "    else:\n",
    "        rr=joblib.load(file_path_pkl)\n",
    "        txt=rr['txt']\n",
    "        meta=rr['meta']\n",
    "        status=rr['status']\n",
    "        eval_time=rr['eval_time']\n",
    "        \n",
    "    final_df.loc[index, 'Status_txt'] = status\n",
    "    final_df.loc[index, 'Length_txt'] = len(txt) if txt is not None else 0    # case of pdf saved as image\n",
    "    final_df.loc[index, 'Length_txt_clean'] = len(txt.replace('\\n','').replace(' ', '')) if txt is not None else 0    # clear whitespace to filter empty files\n",
    "    final_df.at[index, 'Content_txt'] = meta['Content-Type']\n",
    "    final_df.loc[index, 'Metadata'] = [meta]\n",
    "    tot_time+=eval_time\n",
    "        \n",
    "    # save txt\n",
    "    if txt is not None:\n",
    "        final_df.loc[index, 'Path_txt'] = file_path_txt\n",
    "        with open(file_path_txt, 'w') as f:\n",
    "            f.write(unidecode(txt))\n",
    "    \n",
    "    # save checkpoint\n",
    "    if index % 300 == 0 or index == (len(final_df) - 1):\n",
    "        final_df.drop(columns='Metadata').to_csv(os.path.join(CHECKPOINT_FOLDER, 'whitepaper_parsing.csv'), index=False, sep=';')\n",
    "        with open(os.path.join(CHECKPOINT_FOLDER, 'whitepaper_parsing.pickle'), 'wb') as handle:\n",
    "                    pickle.dump(final_df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "print('\\nTotal elapsed time:', str(datetime.timedelta(seconds=round(tot_time))))\n",
    "\n",
    "# save results\n",
    "final_df.drop(columns='Metadata').to_csv(os.path.join(RESULTS_FOLDER, '00b_whitepaper_parsing.csv'), index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6463f9c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Status --\n",
      "\n",
      "OK    3048\n",
      "Name: Status, dtype: int64\n",
      "\n",
      "\n",
      "-- Text length --\n",
      "\n",
      "          Length_txt  Length_txt_clean\n",
      "count    3048.000000       3048.000000\n",
      "mean    22257.032152      17168.373360\n",
      "std     35151.340586      27752.542292\n",
      "min         0.000000          0.000000\n",
      "10%        71.000000         33.000000\n",
      "15%        83.000000         44.000000\n",
      "20%       133.600000         69.000000\n",
      "25%       320.750000        106.000000\n",
      "30%       508.200000        286.200000\n",
      "40%      2323.600000       1264.800000\n",
      "50%      6021.000000       3638.500000\n",
      "75%     34135.500000      26832.750000\n",
      "95%     88365.600000      71564.550000\n",
      "max    485969.000000     411334.000000\n",
      "\n",
      "\n",
      "-- Pdf content --\n",
      "\n",
      "- Single values in \"Content_txt\" (2940):\n",
      "text/html; charset=UTF-8                       1184\n",
      "application/pdf                                1160\n",
      "text/html; charset=ISO-8859-1                   387\n",
      "application/xhtml+xml; charset=UTF-8            133\n",
      "application/xhtml+xml; charset=ISO-8859-1        36\n",
      "application/octet-stream                         12\n",
      "text/plain; charset=ISO-8859-1                   12\n",
      "application/xhtml+xml; charset=windows-1252       5\n",
      "text/html; charset=windows-1252                   4\n",
      "image/png                                         2\n",
      "text/html; charset=EUC-KR                         2\n",
      "text/plain; charset=windows-1252                  1\n",
      "application/xhtml+xml; charset=GB2312             1\n",
      "text/html; charset=Shift_JIS                      1\n",
      "Name: Content_txt, dtype: int64\n",
      "\n",
      "- Multiple values in \"Content_txt\" (108) first column is the number of multiple values in list:\n",
      "2      29\n",
      "9       7\n",
      "3       6\n",
      "11      5\n",
      "4       5\n",
      "19      5\n",
      "8       3\n",
      "18      3\n",
      "26      3\n",
      "21      3\n",
      "23      2\n",
      "15      2\n",
      "41      2\n",
      "42      2\n",
      "14      2\n",
      "31      2\n",
      "5       2\n",
      "24      2\n",
      "20      2\n",
      "7       2\n",
      "47      1\n",
      "35      1\n",
      "104     1\n",
      "80      1\n",
      "326     1\n",
      "165     1\n",
      "52      1\n",
      "12      1\n",
      "13      1\n",
      "10      1\n",
      "89      1\n",
      "37      1\n",
      "6       1\n",
      "61      1\n",
      "36      1\n",
      "201     1\n",
      "27      1\n",
      "387     1\n",
      "38      1\n",
      "Name: aa, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# stats\n",
    "with open(os.path.join(CHECKPOINT_FOLDER, 'whitepaper_parsing.pickle'), 'rb') as handle:\n",
    "    final_df = pickle.load(handle)\n",
    "\n",
    "# status\n",
    "print('-- Status --\\n')\n",
    "print(final_df['Status'].value_counts())\n",
    "# text length\n",
    "print('\\n\\n-- Text length --\\n')\n",
    "print(final_df[['Length_txt', 'Length_txt_clean']].describe(percentiles = [.5, .10, .15, .2, .25, .3, .4, .75, .95]))\n",
    "# pdf content\n",
    "cnt = final_df['Content_txt'].values\n",
    "cnt_single = [x for x in cnt if type(x) == str]\n",
    "cnt_multiple = [x for x in cnt if type(x) != str]\n",
    "print('\\n\\n-- Pdf content --')\n",
    "print('\\n- Single values in \"Content_txt\" (' + str(len(cnt_single)) + '):')\n",
    "print(pd.DataFrame({'Content_txt': cnt_single})['Content_txt'].value_counts())\n",
    "print('\\n- Multiple values in \"Content_txt\" (' + str(len(cnt_multiple)) +') first column is the number of multiple values in list:')\n",
    "print(pd.DataFrame({'aa': [len(x) for x in cnt_multiple]})['aa'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7330fcc8",
   "metadata": {},
   "source": [
    "## Try to recover empty pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a87cd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "LENGTH_TXT_CLEAN_THRSH = 4000     # threshold for maximum non-empty characters in parsed txt file\n",
    "\n",
    "\n",
    "with open(os.path.join(CHECKPOINT_FOLDER, 'whitepaper_parsing.pickle'), 'rb') as handle:\n",
    "    final_df_recover = pickle.load(handle)\n",
    "final_df_recover['Recover_action'] = \"SKIP\"\n",
    "final_df_recover['Recover_Length_txt'] = -1\n",
    "final_df_recover['Recover_Length_txt_clean'] = -1\n",
    "final_df_recover['Recover_Path_txt'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47a1e823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recovering 3048 / 3048 (santiment)                                                                      \n",
      "Total elapsed time: 0:50:42\n"
     ]
    }
   ],
   "source": [
    "URL_ROOT='https://icomarks.com/ico/'    # will be removed from url to create pdf name\n",
    "PATH_TO_WHHTMLTOPDF = r'C:\\Program Files\\wkhtmltopdf\\bin\\wkhtmltopdf.exe' # define path to wkhtmltopdf.exe, see https://python-bloggers.com/2022/06/convert-html-to-pdf-using-python/\n",
    "TEMP_DOWNLOAD_FOLDER = \"C:\\\\Users\\\\Alessandro Bitetto\\\\Downloads\\\\UniPV\\\\ICOs\\\\temp_download\"\n",
    "ID_TO_SKIP = []\n",
    "RELOAD_PDF = True\n",
    "RELOAD_PKL = True\n",
    "\n",
    "# Point pdfkit configuration to wkhtmltopdf.exe\n",
    "config = pdfkit.configuration(wkhtmltopdf=PATH_TO_WHHTMLTOPDF)\n",
    "start = timer()\n",
    "for index, row in final_df_recover.iterrows():\n",
    "\n",
    "    short_name=row['url'].replace(URL_ROOT.replace(\".com\", NEW_DOMAIN), '').replace('|', '')\n",
    "    \n",
    "    print('Recovering ' + str(index + 1) + ' / ' + str(len(final_df_recover)) + ' (' + short_name + ')' +' '*30, end = '\\r')\n",
    "\n",
    "    file_path_pdf = os.path.join(os.getcwd(), RECOVERED_FOLDER, short_name + '.pdf')\n",
    "    file_path_txt = os.path.join(os.getcwd(), RECOVERED_FOLDER, short_name + '.txt')\n",
    "    file_path_pkl = os.path.join(os.getcwd(), RECOVERED_FOLDER, short_name + '.pkl')\n",
    "    original_len_txt = row['Length_txt']\n",
    "    original_len_txt_clean = row['Length_txt_clean']\n",
    "    url = row['WhitepaperUrl']\n",
    "    \n",
    "    # check if txt length is below threshold\n",
    "    if row['Length_txt_clean'] <= LENGTH_TXT_CLEAN_THRSH:\n",
    "        \n",
    "        if short_name in ID_TO_SKIP:\n",
    "            pass\n",
    "        \n",
    "        ##### download pdf from google.drive or dropbox\n",
    "\n",
    "        elif any(x in url for x in ['google', 'goo.gl', 'dropbox']):\n",
    "\n",
    "            source = 'drive' if any(x in url for x in ['google', 'goo.gl']) else 'dropbox'\n",
    "\n",
    "            if not RELOAD_PDF or not os.path.exists(file_path_pdf):\n",
    "            \n",
    "                out = download_from_drive_dropbox(chromedriver_path=CHROMEDRIVER_PATH, download_url=url,\n",
    "                                                  download_folder=TEMP_DOWNLOAD_FOLDER, temp_folder=TEMP_DOWNLOAD_FOLDER,\n",
    "                                                  pdf_name=short_name + '.pdf',\n",
    "                                                  move_folder=os.path.join(os.getcwd(), RECOVERED_FOLDER), source=source)\n",
    "            else:\n",
    "                out=\"ok\"\n",
    "\n",
    "            if out == \"ok\":\n",
    "\n",
    "                # pdf to txt\n",
    "                if not RELOAD_PKL or not os.path.exists(file_path_pkl):\n",
    "        \n",
    "                    # pdf to txt\n",
    "                    start_t = timer()\n",
    "                    txt, meta, parsed_pdf = pdf_to_text(file_path=file_path_pdf, tesseract_path=TESSERACT_PATH, lang='eng')\n",
    "                    status=parsed_pdf['status']\n",
    "                    eval_time=datetime.timedelta(seconds=round(timer()-start_t)).total_seconds()\n",
    "\n",
    "                    # save .pkl\n",
    "                    joblib.dump({'txt': txt, 'meta': meta, 'status': status, 'eval_time': eval_time}, file_path_pkl)\n",
    "\n",
    "                else:\n",
    "                    rr=joblib.load(file_path_pkl)\n",
    "                    txt=rr['txt']\n",
    "                    meta=rr['meta']\n",
    "\n",
    "                len_txt_clean = len(txt.replace('\\n','').replace(' ', '')) if txt is not None else 0 \n",
    "\n",
    "                # update files\n",
    "                if len_txt_clean > original_len_txt_clean:\n",
    "\n",
    "                    final_df_recover.loc[index, 'Recover_action'] = 'DOWNLOAD ' + source.upper() + ' - OK'\n",
    "                    final_df_recover.loc[index, 'Path_Recovered'] = file_path_pdf\n",
    "                    final_df_recover.loc[index, 'Recover_Path_txt'] = file_path_txt\n",
    "                    final_df_recover.loc[index, 'Recover_Length_txt'] = len(txt)\n",
    "                    final_df_recover.loc[index, 'Recover_Length_txt_clean'] = len_txt_clean\n",
    "                    final_df_recover.at[index, 'Content_txt'] = meta['Content-Type']\n",
    "                    final_df_recover.loc[index, 'Metadata'] = [meta]\n",
    "\n",
    "                    # save txt\n",
    "                    with open(file_path_txt, 'w') as f:\n",
    "                        f.write(unidecode(txt))\n",
    "\n",
    "            else:\n",
    "                final_df_recover.loc[index, 'Recover_action'] = 'DOWNLOAD ' + source.upper() + ' - ' + out\n",
    "        \n",
    "        \n",
    "        ##### try to download pdf from html page   e.g. https://www.quasa.io/white-paper\n",
    "        \n",
    "        else:\n",
    "\n",
    "            try:\n",
    "                # Convert Webpage to PDF\n",
    "                if not RELOAD_PDF or not os.path.exists(file_path_pdf):\n",
    "                    def download_web(url, output_path):\n",
    "                        pdfkit.from_url(url, output_path=output_path, configuration=config)\n",
    "                    def run_function(f, max_wait):\n",
    "                        try:\n",
    "                            func_timeout.func_timeout(max_wait, download_web, args=(url, file_path_pdf))\n",
    "                            return 'ok'\n",
    "                        except func_timeout.FunctionTimedOut:\n",
    "                            pass\n",
    "                        return 'timeout'\n",
    "                    out = run_function(download_web, 80)    # stop running after 60*2 seconds\n",
    "                    if out == 'timeout':\n",
    "                        with open(file_path_pdf, 'w') as outfile:     # save empty pdf so speed up when RELOAD_PDF=True\n",
    "                            outfile.write(\"\")\n",
    "\n",
    "                # pdf to txt\n",
    "                if not RELOAD_PKL or not os.path.exists(file_path_pkl):\n",
    "        \n",
    "                    # pdf to txt\n",
    "                    start_t = timer()\n",
    "                    txt, meta, parsed_pdf = pdf_to_text(file_path=file_path_pdf, tesseract_path=TESSERACT_PATH, lang='eng')\n",
    "                    status=parsed_pdf['status']\n",
    "                    eval_time=datetime.timedelta(seconds=round(timer()-start_t)).total_seconds()\n",
    "\n",
    "                    # save .pkl\n",
    "                    joblib.dump({'txt': txt, 'meta': meta, 'status': status, 'eval_time': eval_time}, file_path_pkl)\n",
    "\n",
    "                else:\n",
    "                    rr=joblib.load(file_path_pkl)\n",
    "                    txt=rr['txt']\n",
    "\n",
    "                len_txt_clean = len(txt.replace('\\n','').replace(' ', '')) if txt is not None else 0 \n",
    "\n",
    "                # update files\n",
    "                if len_txt_clean > original_len_txt_clean:\n",
    "\n",
    "                    final_df_recover.loc[index, 'Recover_action'] = \"CONVERT FROM HTML\"\n",
    "                    final_df_recover.loc[index, 'Path_Recovered'] = file_path_pdf\n",
    "                    final_df_recover.loc[index, 'Recover_Path_txt'] = file_path_txt\n",
    "                    final_df_recover.loc[index, 'Recover_Length_txt'] = len(txt)\n",
    "                    final_df_recover.loc[index, 'Recover_Length_txt_clean'] = len_txt_clean\n",
    "                    final_df_recover.at[index, 'Content_txt'] = meta['Content-Type']\n",
    "                    final_df_recover.loc[index, 'Metadata'] = [meta]\n",
    "\n",
    "                    # save txt\n",
    "                    with open(file_path_txt, 'w') as f:\n",
    "                        f.write(unidecode(txt))\n",
    "            except:\n",
    "                final_df_recover.loc[index, 'Recover_action'] = \"CONVERT FROM HTML - FAILED\"\n",
    "\n",
    "    else:\n",
    "        final_df_recover.loc[index, 'Recover_action'] = \"KEEP ORIGINAL\"\n",
    "        \n",
    "    # save checkpoint\n",
    "    if index % 300 == 0 or index == (len(final_df_recover) - 1):\n",
    "        final_df_recover.drop(columns='Metadata').to_csv(os.path.join(CHECKPOINT_FOLDER, 'whitepaper_recover.csv'), index=False, sep=';')\n",
    "        with open(os.path.join(CHECKPOINT_FOLDER,'whitepaper_recover.pickle'), 'wb') as handle:\n",
    "                    pickle.dump(final_df_recover, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "print('\\nTotal elapsed time:', str(datetime.timedelta(seconds=round(timer()-start))))\n",
    "\n",
    "# save results\n",
    "final_df_recover.drop(columns='Metadata').to_csv(os.path.join(RESULTS_FOLDER, '00c_whitepaper_recover.csv'), index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b88d01b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>WhitepaperUrl</th>\n",
       "      <th>Status</th>\n",
       "      <th>Error</th>\n",
       "      <th>Path_Original</th>\n",
       "      <th>Path_Recovered</th>\n",
       "      <th>Path_txt</th>\n",
       "      <th>Status_txt</th>\n",
       "      <th>Length_txt</th>\n",
       "      <th>Length_txt_clean</th>\n",
       "      <th>Content_txt</th>\n",
       "      <th>Metadata</th>\n",
       "      <th>Recover_action</th>\n",
       "      <th>Recover_Length_txt</th>\n",
       "      <th>Recover_Length_txt_clean</th>\n",
       "      <th>Recover_Path_txt</th>\n",
       "      <th>Final_Path_txt</th>\n",
       "      <th>Final_Length_txt</th>\n",
       "      <th>Final_Length_txt_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://icomarks.ai/ico/the-mill-of-blood</td>\n",
       "      <td>https://millofblood.com/white-paper.php</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td></td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td>200</td>\n",
       "      <td>23152.0</td>\n",
       "      <td>8655.0</td>\n",
       "      <td>text/html; charset=UTF-8</td>\n",
       "      <td>{'Content-Encoding': 'UTF-8', 'Content-Languag...</td>\n",
       "      <td>KEEP ORIGINAL</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td>23152</td>\n",
       "      <td>8655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://icomarks.ai/ico/xenchain</td>\n",
       "      <td>https://cryptototem.com/wp-ico/img/files/65tpX...</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td>200</td>\n",
       "      <td>316.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>text/html; charset=ISO-8859-1</td>\n",
       "      <td>{'Content-Encoding': 'ISO-8859-1', 'Content-La...</td>\n",
       "      <td>FROM CRYPTOTOTEM</td>\n",
       "      <td>65927</td>\n",
       "      <td>53136</td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td>65928</td>\n",
       "      <td>53136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://icomarks.ai/ico/moonlight</td>\n",
       "      <td>https://cryptototem.com/wp-ico/img/files/ANkwp...</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td></td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td>200</td>\n",
       "      <td>64263.0</td>\n",
       "      <td>53667.0</td>\n",
       "      <td>application/pdf</td>\n",
       "      <td>{'Author': 'moonlight.io', 'Content-Type': 'ap...</td>\n",
       "      <td>FROM CRYPTOTOTEM</td>\n",
       "      <td>64638</td>\n",
       "      <td>53980</td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td>64639</td>\n",
       "      <td>53980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://icomarks.ai/ico/shipnext</td>\n",
       "      <td>https://cryptototem.com/wp-ico/img/files/0DAC0...</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td></td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td>200</td>\n",
       "      <td>3841.0</td>\n",
       "      <td>3157.0</td>\n",
       "      <td>text/html; charset=UTF-8</td>\n",
       "      <td>{'Content-Encoding': 'UTF-8', 'Content-Languag...</td>\n",
       "      <td>FROM CRYPTOTOTEM</td>\n",
       "      <td>82113</td>\n",
       "      <td>68250</td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td>82114</td>\n",
       "      <td>68250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         url  \\\n",
       "0  https://icomarks.ai/ico/the-mill-of-blood   \n",
       "1           https://icomarks.ai/ico/xenchain   \n",
       "2          https://icomarks.ai/ico/moonlight   \n",
       "3           https://icomarks.ai/ico/shipnext   \n",
       "\n",
       "                                       WhitepaperUrl Status Error  \\\n",
       "0            https://millofblood.com/white-paper.php     OK   NaN   \n",
       "1  https://cryptototem.com/wp-ico/img/files/65tpX...     OK   NaN   \n",
       "2  https://cryptototem.com/wp-ico/img/files/ANkwp...     OK   NaN   \n",
       "3  https://cryptototem.com/wp-ico/img/files/0DAC0...     OK   NaN   \n",
       "\n",
       "                                       Path_Original  \\\n",
       "0  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...   \n",
       "1  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...   \n",
       "2  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...   \n",
       "3  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...   \n",
       "\n",
       "                                      Path_Recovered  \\\n",
       "0                                                      \n",
       "1  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...   \n",
       "2                                                      \n",
       "3                                                      \n",
       "\n",
       "                                            Path_txt Status_txt  Length_txt  \\\n",
       "0  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...        200     23152.0   \n",
       "1  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...        200       316.0   \n",
       "2  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...        200     64263.0   \n",
       "3  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...        200      3841.0   \n",
       "\n",
       "   Length_txt_clean                    Content_txt  \\\n",
       "0            8655.0       text/html; charset=UTF-8   \n",
       "1              88.0  text/html; charset=ISO-8859-1   \n",
       "2           53667.0                application/pdf   \n",
       "3            3157.0       text/html; charset=UTF-8   \n",
       "\n",
       "                                            Metadata    Recover_action  \\\n",
       "0  {'Content-Encoding': 'UTF-8', 'Content-Languag...     KEEP ORIGINAL   \n",
       "1  {'Content-Encoding': 'ISO-8859-1', 'Content-La...  FROM CRYPTOTOTEM   \n",
       "2  {'Author': 'moonlight.io', 'Content-Type': 'ap...  FROM CRYPTOTOTEM   \n",
       "3  {'Content-Encoding': 'UTF-8', 'Content-Languag...  FROM CRYPTOTOTEM   \n",
       "\n",
       "   Recover_Length_txt  Recover_Length_txt_clean  \\\n",
       "0                  -1                        -1   \n",
       "1               65927                     53136   \n",
       "2               64638                     53980   \n",
       "3               82113                     68250   \n",
       "\n",
       "                                    Recover_Path_txt  \\\n",
       "0                                                      \n",
       "1  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...   \n",
       "2  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...   \n",
       "3  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...   \n",
       "\n",
       "                                      Final_Path_txt Final_Length_txt  \\\n",
       "0  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...            23152   \n",
       "1  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...            65928   \n",
       "2  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...            64639   \n",
       "3  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...            82114   \n",
       "\n",
       "   Final_Length_txt_clean  \n",
       "0                    8655  \n",
       "1                   53136  \n",
       "2                   53980  \n",
       "3                   68250  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recover actions stats\n",
    "with open(os.path.join(CHECKPOINT_FOLDER,'whitepaper_recover.pickle'), 'rb') as handle:\n",
    "    final_df_recover = pickle.load(handle)\n",
    "final_df_recover.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f5ce9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Recover actions --\n",
      "\n",
      "KEEP ORIGINAL                            1096\n",
      "FROM CRYPTOTOTEM                          823\n",
      "SKIP                                      440\n",
      "CONVERT FROM HTML                         322\n",
      "FROM SECOND DOWNLOAD                      164\n",
      "DOWNLOAD DRIVE - OK                       158\n",
      "DOWNLOAD DRIVE - page not available       106\n",
      "CONVERT FROM HTML - FAILED                 80\n",
      "DOWNLOAD DROPBOX - OK                      11\n",
      "DOWNLOAD DROPBOX - page not available       9\n",
      "DOWNLOAD DRIVE - out of time                2\n",
      "DOWNLOAD DROPBOX - out of time              1\n",
      "Name: Recover_action, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# recover actions stats\n",
    "with open(os.path.join(CHECKPOINT_FOLDER,'whitepaper_recover.pickle'), 'rb') as handle:\n",
    "    final_df_recover = pickle.load(handle)\n",
    "\n",
    "final_df_recover['url']=final_df_recover['url'].str.replace(\".com\", NEW_DOMAIN, regex=False)\n",
    "\n",
    "print('-- Recover actions --\\n')\n",
    "print(final_df_recover['Recover_action'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97fd92f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total available whitepapers: 2574\n",
      "   - with more than 4000 clean characters: 2202\n"
     ]
    }
   ],
   "source": [
    "# set final path for txt files\n",
    "RECOVER_TO_KEEP = ['CONVERT FROM HTML', 'DOWNLOAD DRIVE - OK', 'DOWNLOAD DROPBOX - OK', 'FROM CRYPTOTOTEM', 'FROM SECOND DOWNLOAD']\n",
    "LENGTH_TXT_CLEAN_THRSH = 4000     # threshold for maximum non-empty characters in parsed txt file\n",
    "\n",
    "final_df_recover['Final_Path_txt'] = np.where(final_df_recover['Recover_action'] == 'KEEP ORIGINAL', final_df_recover['Path_txt'], '')\n",
    "final_df_recover['Final_Path_txt'] = np.where(final_df_recover['Recover_action'].isin(RECOVER_TO_KEEP), final_df_recover['Recover_Path_txt'], final_df_recover['Final_Path_txt'])\n",
    "final_df_recover['Final_Length_txt'] = np.where(final_df_recover['Recover_action'] == 'KEEP ORIGINAL', final_df_recover['Length_txt'], '')\n",
    "final_df_recover['Final_Length_txt'] = np.where(final_df_recover['Recover_action'].isin(RECOVER_TO_KEEP), final_df_recover['Recover_Length_txt'], final_df_recover['Final_Length_txt'])\n",
    "final_df_recover['Final_Length_txt_clean'] = np.where(final_df_recover['Recover_action'] == 'KEEP ORIGINAL', final_df_recover['Length_txt_clean'], -1)\n",
    "final_df_recover['Final_Length_txt_clean'] = np.where(final_df_recover['Recover_action'].isin(RECOVER_TO_KEEP), final_df_recover['Recover_Length_txt_clean'], final_df_recover['Final_Length_txt_clean'])\n",
    "\n",
    "\n",
    "final_df_recover.drop(columns='Metadata').to_csv(os.path.join(RESULTS_FOLDER, '00d_whitepaper_final.csv'), index=False, sep=';')\n",
    "\n",
    "print('Total available whitepapers:', sum(final_df_recover['Final_Path_txt'] != ''))\n",
    "print(f'   - with more than {LENGTH_TXT_CLEAN_THRSH} clean characters:', sum(final_df_recover['Final_Length_txt_clean'] >= LENGTH_TXT_CLEAN_THRSH))\n",
    "with open(os.path.join(CHECKPOINT_FOLDER,'whitepaper_final.pickle'), 'wb') as handle:\n",
    "    pickle.dump(final_df_recover, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06810293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 2301 / 2301\n",
      "Total elapsed time: 0:00:53\n"
     ]
    }
   ],
   "source": [
    "import pycld2 as cld2\n",
    "import regex\n",
    "\n",
    "\n",
    "# load text\n",
    "LENGTH_TXT_CLEAN_THRSH = 2000     # threshold for maximum non-empty characters in parsed txt file\n",
    "\n",
    "with open(os.path.join(CHECKPOINT_FOLDER, 'whitepaper_recover.pickle'), 'rb') as handle:\n",
    "    final_df = pickle.load(handle)\n",
    "\n",
    "df_text = (final_df[final_df['Final_Length_txt_clean'] >= LENGTH_TXT_CLEAN_THRSH][['url', 'Final_Length_txt', 'Final_Length_txt_clean', 'Final_Path_txt']])\n",
    "df_text = df_text[df_text['Final_Path_txt'] != ''].reset_index(drop = True)\n",
    "df_text['text'] = ''\n",
    "\n",
    "start = timer()\n",
    "for index, row in df_text.iterrows():\n",
    "    \n",
    "    print('Reading ' + str(index + 1) + ' / ' + str(len(df_text)), end = '\\r')\n",
    "    \n",
    "    with open(row['Final_Path_txt']) as f:\n",
    "        df_text.loc[index, 'text'] = f.read()\n",
    "print('\\nTotal elapsed time:', str(datetime.timedelta(seconds=round(timer()-start))))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5baeffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url                          https://icomarks.ai/ico/hanuman-universe-token\n",
       "Final_Length_txt                                                       3384\n",
       "Final_Length_txt_clean                                                 2701\n",
       "Final_Path_txt            C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...\n",
       "text                      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...\n",
       "Name: 2300, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "494611fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing 2301 / 2301   last interaction: 18/03/2024 18:43:38\n",
      "\n",
      "Total elapsed time: 0:00:00\n",
      "\n",
      "Data saved in .\\Results\\00e_whitepaper_language_report.csv\n"
     ]
    }
   ],
   "source": [
    "RE_BAD_CHARS = regex.compile(r\"[\\p{Cc}\\p{Cs}]+\")\n",
    "def remove_bad_chars(text):\n",
    "    return RE_BAD_CHARS.sub(\"\", text)\n",
    "\n",
    "df_report=pd.DataFrame()\n",
    "tot_time=0\n",
    "for index, row in df_text.iterrows():\n",
    "    \n",
    "    print(f'Parsing {index+1} / {len(df_text)}   last interaction: {datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}', end = '\\r')\n",
    "    \n",
    "    text = row['text']\n",
    "    start=timer()\n",
    "    isReliable, _, details = cld2.detect(remove_bad_chars(text), returnVectors=False)\n",
    "    add_row=pd.DataFrame({'url': row['url'],\n",
    "                          'reliable': isReliable,\n",
    "                          'total_page_lang': len([x for x in details if x[0] != 'Unknown'])}, index=[index])\n",
    "    add_row['Lang1']=['undetected']\n",
    "    for i, v in enumerate(details):\n",
    "        if v[0] != 'Unknown':\n",
    "            add_row['Lang'+str(i+1)]=[v[0]]\n",
    "            add_row['Lang'+str(i+1)+'_accuracy']=[v[2]]\n",
    "            add_row['Lang'+str(i+1)+'_score']=[v[3]]\n",
    "    \n",
    "    eval_time=datetime.timedelta(seconds=round(timer()-start)).total_seconds()\n",
    "    add_row['eval_time']=eval_time\n",
    "    lang_preval=(add_row['Lang1'].value_counts().to_frame() / add_row.shape[0]).sort_values(by='Lang1', ascending=False)\n",
    "    add_row.insert(add_row.columns.get_loc(\"total_page_lang\")+1, 'Lang1_preval', lang_preval.index[0])\n",
    "    add_row.insert(add_row.columns.get_loc(\"Lang1_preval\")+1, 'Lang1_preval_perc', lang_preval['Lang1'][0])\n",
    "    add_row['Final_Path_txt']=row['Final_Path_txt']\n",
    "    tot_time+=eval_time\n",
    "    df_report=pd.concat([df_report, add_row])\n",
    "\n",
    "move_col = df_report.pop('Final_Path_txt')\n",
    "df_report.insert(df_report.shape[1], 'Final_Path_txt', move_col)\n",
    "    \n",
    "print('\\n\\nTotal elapsed time:', str(datetime.timedelta(seconds=round(tot_time))))\n",
    "\n",
    "df_report.to_csv(os.path.join(RESULTS_FOLDER, '00e_whitepaper_language_report.csv'), index=False, sep=';')\n",
    "print('\\nData saved in', os.path.join(RESULTS_FOLDER, '00e_whitepaper_language_report.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0ec74bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prevalent Language</th>\n",
       "      <th>Number of documents</th>\n",
       "      <th>Minimum Prevalence %</th>\n",
       "      <th>5th Percentile Prevalence %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>2215</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>undetected</td>\n",
       "      <td>33</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DANISH</td>\n",
       "      <td>11</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>INDONESIAN</td>\n",
       "      <td>7</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SPANISH</td>\n",
       "      <td>6</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GERMAN</td>\n",
       "      <td>5</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PORTUGUESE</td>\n",
       "      <td>4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>IRISH</td>\n",
       "      <td>4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SERBIAN</td>\n",
       "      <td>3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ITALIAN</td>\n",
       "      <td>3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NORWEGIAN</td>\n",
       "      <td>2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DUTCH</td>\n",
       "      <td>2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LATIN</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FRENCH</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FINNISH</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SESOTHO</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ZULU</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BASQUE</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Prevalent Language  Number of documents  Minimum Prevalence %  \\\n",
       "3             ENGLISH                 2215                 100.0   \n",
       "17         undetected                   33                 100.0   \n",
       "1              DANISH                   11                 100.0   \n",
       "7          INDONESIAN                    7                 100.0   \n",
       "15            SPANISH                    6                 100.0   \n",
       "6              GERMAN                    5                 100.0   \n",
       "12         PORTUGUESE                    4                 100.0   \n",
       "8               IRISH                    4                 100.0   \n",
       "13            SERBIAN                    3                 100.0   \n",
       "9             ITALIAN                    3                 100.0   \n",
       "11          NORWEGIAN                    2                 100.0   \n",
       "2               DUTCH                    2                 100.0   \n",
       "10              LATIN                    1                 100.0   \n",
       "5              FRENCH                    1                 100.0   \n",
       "4             FINNISH                    1                 100.0   \n",
       "14            SESOTHO                    1                 100.0   \n",
       "16               ZULU                    1                 100.0   \n",
       "0              BASQUE                    1                 100.0   \n",
       "\n",
       "    5th Percentile Prevalence %  \n",
       "3                         100.0  \n",
       "17                        100.0  \n",
       "1                         100.0  \n",
       "7                         100.0  \n",
       "15                        100.0  \n",
       "6                         100.0  \n",
       "12                        100.0  \n",
       "8                         100.0  \n",
       "13                        100.0  \n",
       "9                         100.0  \n",
       "11                        100.0  \n",
       "2                         100.0  \n",
       "10                        100.0  \n",
       "5                         100.0  \n",
       "4                         100.0  \n",
       "14                        100.0  \n",
       "16                        100.0  \n",
       "0                         100.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reliable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>2238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reliable\n",
       "True       2238\n",
       "False        63"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details of \"reliable\"==False:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lang1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>undetected</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENGLISH</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IRISH</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SERBIAN</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DUTCH</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GERMAN</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDONESIAN</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DANISH</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZULU</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPANISH</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SESOTHO</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Lang1\n",
       "undetected     33\n",
       "ENGLISH        14\n",
       "IRISH           4\n",
       "SERBIAN         3\n",
       "DUTCH           2\n",
       "GERMAN          2\n",
       "INDONESIAN      1\n",
       "DANISH          1\n",
       "ZULU            1\n",
       "SPANISH         1\n",
       "SESOTHO         1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_report = pd.read_csv(os.path.join(RESULTS_FOLDER, '00e_whitepaper_language_report.csv'), sep=';')\n",
    "\n",
    "display(\n",
    "(df_report[['url', 'Lang1_preval', 'Lang1_preval_perc']]\n",
    " .drop_duplicates()\n",
    " .groupby(['url', 'Lang1_preval'], as_index=False)\n",
    " .min()\n",
    " .drop(columns='url')\n",
    " .groupby('Lang1_preval', as_index=False)\n",
    " .agg(Tot_Documents = ('Lang1_preval', lambda x: len(x)),\n",
    "      Min_Prevalence = ('Lang1_preval_perc', lambda x: round(min(x)*100, 2)),\n",
    "      Min5_Prevalence = ('Lang1_preval_perc', lambda x: round(np.quantile(x, 0.05)*100, 2))) \n",
    " .rename(columns={'Lang1_preval': 'Prevalent Language', 'Tot_Documents': 'Number of documents',\n",
    "                  'Min_Prevalence': 'Minimum Prevalence %', 'Min5_Prevalence': '5th Percentile Prevalence %'})\n",
    " .sort_values(by='Number of documents', ascending=False)\n",
    "))\n",
    "\n",
    "display(df_report['reliable'].value_counts().to_frame())\n",
    "print('Details of \"reliable\"==False:')\n",
    "display(df_report[df_report['reliable']==False]['Lang1'].value_counts().to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ad5aa09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>reliable</th>\n",
       "      <th>total_page_lang</th>\n",
       "      <th>Lang1_preval</th>\n",
       "      <th>Lang1_preval_perc</th>\n",
       "      <th>Lang1</th>\n",
       "      <th>Lang1_accuracy</th>\n",
       "      <th>Lang1_score</th>\n",
       "      <th>Lang2</th>\n",
       "      <th>Lang2_accuracy</th>\n",
       "      <th>Lang2_score</th>\n",
       "      <th>eval_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>https://icomarks.ai/ico/hanuman-universe-token</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>94</td>\n",
       "      <td>908.0</td>\n",
       "      <td>LATIN</td>\n",
       "      <td>5</td>\n",
       "      <td>915.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  reliable  \\\n",
       "2300  https://icomarks.ai/ico/hanuman-universe-token      True   \n",
       "\n",
       "      total_page_lang Lang1_preval  Lang1_preval_perc    Lang1  \\\n",
       "2300                2      ENGLISH                1.0  ENGLISH   \n",
       "\n",
       "      Lang1_accuracy  Lang1_score  Lang2  Lang2_accuracy  Lang2_score  \\\n",
       "2300              94        908.0  LATIN               5        915.0   \n",
       "\n",
       "      eval_time  \n",
       "2300        0.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_row['eval_time']=eval_time\n",
    "lang_preval=(add_row['Lang1'].value_counts().to_frame() / add_row.shape[0]).sort_values(by='Lang1', ascending=False)\n",
    "add_row.insert(add_row.columns.get_loc(\"total_page_lang\")+1, 'Lang1_preval', lang_preval.index[0])\n",
    "add_row.insert(add_row.columns.get_loc(\"Lang1_preval\")+1, 'Lang1_preval_perc', lang_preval['Lang1'][0])\n",
    "add_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff904f77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "414a5a26",
   "metadata": {},
   "source": [
    "## Recover missing pdf with Cryptototem url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "405b2fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(CHECKPOINT_FOLDER, 'whitepaper_final.pickle'), 'rb') as handle:\n",
    "    final_df = pickle.load(handle)\n",
    "\n",
    "robin = pd.read_csv('IcoDataRobin.csv', sep = \";\")[['url', 'pdfWP']].drop_duplicates()\n",
    "dd = final_df[['url', 'Final_Path_txt', 'Final_Length_txt' ,'Final_Length_txt_clean']].add_suffix('_original').rename(columns={'url_original': 'url'}).merge(robin, on='url', how='left')\n",
    "dd = dd[~dd['pdfWP'].isna()]\n",
    "# dd.to_csv('dd.csv', index=False, sep=';')\n",
    "# dd = dd[dd['Final_Length_txt_clean_original'] <= 10000]\n",
    "dd.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0da7ee76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 981 / 981  - Total OK: 977\n",
      "Total elapsed time: 0:58:33\n"
     ]
    }
   ],
   "source": [
    "# Parse pdf and convert to txt\n",
    "HEADERS = {\"User-Agent\": \"Chrome/51.0.2704.103\"}\n",
    "URL_ROOT='https://icomarks.com/ico/'    # will be removed from url to create pdf name\n",
    "RELOAD_PKL=True\n",
    "\n",
    "OK_count = 0\n",
    "tot_time = 0\n",
    "dd['Status'] = ''\n",
    "dd['Error'] = ''\n",
    "dd['Final_Path_txt'] = ''\n",
    "dd['Final_Length_txt'] = ''\n",
    "dd['Final_Length_txt_clean'] = ''\n",
    "for index, row in dd.iterrows():\n",
    "    \n",
    "    url = row['pdfWP']\n",
    "    file_name = os.path.join(RECOVERED_FOLDER, row['url'].replace(URL_ROOT.replace(\".com\", NEW_DOMAIN), '').replace('|', '') + '.pdf')\n",
    "    file_path_txt = os.path.join(os.getcwd(), RECOVERED_FOLDER, row['url'].replace(URL_ROOT.replace(\".com\", NEW_DOMAIN), '').replace('|', '') + '.txt')\n",
    "    file_path_pkl = os.path.join(os.getcwd(), RECOVERED_FOLDER, row['url'].replace(URL_ROOT.replace(\".com\", NEW_DOMAIN), '').replace('|', '') + '.pkl')\n",
    "\n",
    "    print('Downloading ' + str(index + 1) + ' / ' + str(len(dd)) + '  - Total OK: ' + str(OK_count), end = '\\r')\n",
    "    \n",
    "    try:\n",
    "        # connect\n",
    "        response = requests.get(url, headers = HEADERS)\n",
    "\n",
    "        # check response and save pdf\n",
    "        if response.status_code == 200:\n",
    "            with open(file_name, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            dd.loc[index, 'Status'] = 'OK'\n",
    "            OK_count += 1\n",
    "        else:\n",
    "            dd.loc[index, 'Status'] = response.status_code\n",
    "\n",
    "    except Exception as e:\n",
    "        dd.loc[index, 'Status'] = 'ERROR'\n",
    "        dd.loc[index, 'Error'] = e\n",
    "        \n",
    "        \n",
    "    if response.status_code == 200:\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            # pdf to txt\n",
    "            start = timer()\n",
    "            txt, meta, parsed_pdf = pdf_to_text(file_path=file_name, tesseract_path=TESSERACT_PATH, lang='eng')\n",
    "            status=parsed_pdf['status']\n",
    "            eval_time=datetime.timedelta(seconds=round(timer()-start)).total_seconds()\n",
    "            tot_time+=eval_time\n",
    "\n",
    "            clean_text = len(txt.replace('\\n','').replace(' ', '')) if txt is not None else 0    # clear whitespace to filter empty files\n",
    "            dd.loc[index, 'Final_Length_txt'] = len(txt) if txt is not None else 0    # case of pdf saved as image\n",
    "            dd.loc[index, 'Final_Length_txt_clean'] = clean_text\n",
    "\n",
    "            if clean_text > row['Final_Length_txt_clean_original']:\n",
    "\n",
    "                # save .pkl\n",
    "                joblib.dump({'txt': txt, 'meta': meta, 'status': status, 'eval_time': eval_time}, file_path_pkl)\n",
    "\n",
    "                # save txt\n",
    "                if txt is not None:\n",
    "                    dd.loc[index, 'Final_Path_txt'] = file_path_txt\n",
    "                    with open(file_path_txt, 'w') as f:\n",
    "                        f.write(unidecode(txt))\n",
    "\n",
    "            else:\n",
    "                dd.loc[index, 'Status'] = 'SKIPPED'\n",
    "                \n",
    "        except Exception as e:\n",
    "            dd.loc[index, 'Status'] = 'ERROR'\n",
    "            dd.loc[index, 'Error'] = e\n",
    "                \n",
    "print('\\nTotal elapsed time:', str(datetime.timedelta(seconds=round(tot_time))))\n",
    "\n",
    "dd.to_csv(os.path.join(RESULTS_FOLDER, '00zz_whitepaper_recover_Robin.csv'), index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0928896e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>OK</th>\n",
       "      <td>821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SKIPPED</th>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ERROR</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Status\n",
       "OK          821\n",
       "SKIPPED     155\n",
       "ERROR         3\n",
       "404           1\n",
       "520           1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Status of recovered\n",
    "dd = pd.read_csv(os.path.join(RESULTS_FOLDER, '00zz_whitepaper_recover_Robin.csv'), sep = \";\")\n",
    "display(dd['Status'].value_counts().to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fff1664",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(CHECKPOINT_FOLDER,'whitepaper_recover.pickle'), 'rb') as handle:\n",
    "    final_df_recover = pickle.load(handle)\n",
    "\n",
    "dd = pd.read_csv(os.path.join(RESULTS_FOLDER, '00zz_whitepaper_recover_Robin.csv'), sep = \";\")\n",
    "dd = dd[dd['Status'] == 'OK']\n",
    "\n",
    "for index, row in final_df_recover.iterrows():\n",
    "    \n",
    "    if row['url'] in dd['url'].values:\n",
    "        ref_dd = dd[dd['url'] == row['url']]\n",
    "        final_df_recover.loc[index, 'Status'] = 'OK'\n",
    "        final_df_recover.loc[index, 'Recover_action'] = 'FROM CRYPTOTOTEM'\n",
    "        final_df_recover.loc[index, 'WhitepaperUrl'] = ref_dd['pdfWP'].values[0]\n",
    "        final_df_recover.loc[index, 'Recover_Length_txt'] = int(ref_dd['Final_Length_txt'].values[0])\n",
    "        final_df_recover.loc[index, 'Recover_Length_txt_clean'] = int(ref_dd['Final_Length_txt_clean'].values[0])\n",
    "        final_df_recover.loc[index, 'Recover_Path_txt'] = ref_dd['Final_Path_txt'].values[0]\n",
    "        \n",
    "with open(os.path.join(CHECKPOINT_FOLDER,'whitepaper_recover.pickle'), 'wb') as handle:\n",
    "    pickle.dump(final_df_recover, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6e41c6",
   "metadata": {},
   "source": [
    "## Recover whitepaper from second download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31e915dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_new = pd.read_csv(os.path.join(RESULTS_FOLDER, '01b_ICOmarks_ico_list_new_only.csv'), sep=';')['url']\n",
    "# dd = pd.read_pickle(os.path.join(CHECKPOINT_FOLDER, 'formatted_df.pkl'))\n",
    "dd = pd.read_csv(os.path.join(RESULTS_FOLDER, '01g_ICOmarks_ico_list_scraped_formatted.csv'), sep = \";\")\n",
    "dd = dd[dd['url'].isin(url_new)]\n",
    "dd = dd[~dd['WhitepaperUrl'].isna()][['url', 'WhitepaperUrl']]\n",
    "dd = dd.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cceb8031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 233 / 233      hanuman-universe-token                                                 \n",
      "Total elapsed time: 0:34:21\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>OK FROM REQUESTOK PDF TO TXT</th>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ERROR</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OK ALREADY DOWNLOADEDOK PDF TO TXT</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OK ALREADY DOWNLOADED - ERROR IN PDF TO TXT</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOWNLOAD DRIVE - OKOK PDF TO TXT</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>page not available</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Status\n",
       "OK FROM REQUESTOK PDF TO TXT                    151\n",
       "ERROR                                            33\n",
       "404                                              19\n",
       "OK ALREADY DOWNLOADEDOK PDF TO TXT               10\n",
       "403                                               5\n",
       "OK ALREADY DOWNLOADED - ERROR IN PDF TO TXT       4\n",
       "522                                               3\n",
       "DOWNLOAD DRIVE - OKOK PDF TO TXT                  3\n",
       "500                                               2\n",
       "page not available                                2\n",
       "523                                               1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parse pdf and convert to txt\n",
    "HEADERS = {\"User-Agent\": \"Chrome/51.0.2704.103\"}\n",
    "URL_ROOT='https://icomarks.com/ico/'    # will be removed from url to create pdf name\n",
    "RELOAD_PKL=True\n",
    "TEMP_DOWNLOAD_FOLDER = \"C:\\\\Users\\\\Alessandro Bitetto\\\\Downloads\\\\UniPV\\\\ICOs\\\\temp_download\"\n",
    "\n",
    "tot_time = 0\n",
    "dd['Status'] = ''\n",
    "dd['Error'] = ''\n",
    "dd['Final_Path_txt'] = ''\n",
    "dd['Final_Length_txt'] = ''\n",
    "dd['Final_Length_txt_clean'] = ''\n",
    "for index, row in dd.iterrows():\n",
    "    \n",
    "    url = row['WhitepaperUrl']\n",
    "    file_name = os.path.join(RECOVERED_FOLDER, row['url'].replace(URL_ROOT.replace(\".com\", NEW_DOMAIN), '').replace('|', '') + '.pdf')\n",
    "    file_path_txt = os.path.join(os.getcwd(), RECOVERED_FOLDER, row['url'].replace(URL_ROOT.replace(\".com\", NEW_DOMAIN), '').replace('|', '') + '.txt')\n",
    "    file_path_pkl = os.path.join(os.getcwd(), RECOVERED_FOLDER, row['url'].replace(URL_ROOT.replace(\".com\", NEW_DOMAIN), '').replace('|', '') + '.pkl')\n",
    "\n",
    "    print('Downloading ' + str(index + 1) + ' / ' + str(len(dd)) + '      '+ row['url'].replace(URL_ROOT.replace(\".com\", NEW_DOMAIN), '').replace('|', '') + ' '*30, end = '\\r')\n",
    "    \n",
    "    eval_time=0\n",
    "    error=''\n",
    "    txt=None\n",
    "    if not RELOAD_PKL or not os.path.exists(file_path_pkl):\n",
    "    \n",
    "        try:\n",
    "            # try to download from Dropbox or Drive\n",
    "            out=''\n",
    "            if any(x in url for x in ['google', 'goo.gl', 'dropbox']):\n",
    "\n",
    "                source = 'drive' if any(x in url for x in ['google', 'goo.gl']) else 'dropbox'\n",
    "\n",
    "                status = 'OK ALREADY DOWNLOADED'\n",
    "                if not os.path.exists(file_name):\n",
    "\n",
    "                    out = download_from_drive_dropbox(chromedriver_path=CHROMEDRIVER_PATH, download_url=url,\n",
    "                                                      download_folder=TEMP_DOWNLOAD_FOLDER, temp_folder=TEMP_DOWNLOAD_FOLDER,\n",
    "                                                      pdf_name=os.path.basename(file_name),\n",
    "                                                      move_folder=os.path.join(os.getcwd(), RECOVERED_FOLDER), source=source)\n",
    "                    if out != 'ok':\n",
    "                        status = out\n",
    "                    else:\n",
    "                        status = 'DOWNLOAD ' + source.upper() + ' - OK'\n",
    "\n",
    "            else:\n",
    "                status = 'OK ALREADY DOWNLOADED'\n",
    "                if not os.path.exists(file_name):\n",
    "                    # connect\n",
    "                    response = requests.get(url, headers = HEADERS)\n",
    "\n",
    "                    # check response and save pdf\n",
    "                    if response.status_code == 200:\n",
    "                        with open(file_name, \"wb\") as f:\n",
    "                            f.write(response.content)\n",
    "                        status = 'OK FROM REQUEST'\n",
    "                    else:\n",
    "                        status = str(response.status_code)\n",
    "\n",
    "            if 'OK' in status:\n",
    "\n",
    "                try:\n",
    "                    # pdf to txt\n",
    "                    start = timer()\n",
    "                    txt, meta, parsed_pdf = pdf_to_text(file_path=file_name, tesseract_path=TESSERACT_PATH, lang='eng')\n",
    "                    status_pars=parsed_pdf['status']\n",
    "                    if status_pars == 200:\n",
    "                        status += 'OK PDF TO TXT'\n",
    "                    else:\n",
    "                        status += str(status_pars)\n",
    "                    eval_time=datetime.timedelta(seconds=round(timer()-start)).total_seconds()\n",
    "\n",
    "                    # save .pkl\n",
    "                    joblib.dump({'txt': txt, 'meta': meta, 'status': status, 'eval_time': eval_time}, file_path_pkl)\n",
    "\n",
    "                except Exception as e:\n",
    "                    status += ' - ERROR IN PDF TO TXT'\n",
    "                    error = e\n",
    "                    \n",
    "        except Exception as e:\n",
    "            status = 'ERROR'\n",
    "            error = e\n",
    "    \n",
    "    else:\n",
    "        rr=joblib.load(file_path_pkl)\n",
    "        txt=rr['txt']\n",
    "        status=rr['status']\n",
    "        eval_time=rr['eval_time']\n",
    "\n",
    "    \n",
    "    dd.loc[index, 'Status'] = status\n",
    "    dd.loc[index, 'Error'] = error\n",
    "    clean_text = len(txt.replace('\\n','').replace(' ', '')) if txt is not None else 0    # clear whitespace to filter empty files\n",
    "    dd.loc[index, 'Final_Length_txt'] = len(txt) if txt is not None else 0    # case of pdf saved as image\n",
    "    dd.loc[index, 'Final_Length_txt_clean'] = clean_text\n",
    "    \n",
    "    # save txt\n",
    "    if txt is not None:\n",
    "        dd.loc[index, 'Final_Path_txt'] = file_path_txt\n",
    "        with open(file_path_txt, 'w') as f:\n",
    "            f.write(unidecode(txt))\n",
    "                            \n",
    "    tot_time+=eval_time\n",
    "                \n",
    "print('\\nTotal elapsed time:', str(datetime.timedelta(seconds=round(tot_time))))\n",
    "\n",
    "dd.to_csv(os.path.join(RESULTS_FOLDER, '00zz_whitepaper_recover_second_download.csv'), index=False, sep=';')\n",
    "\n",
    "display(dd['Status'].value_counts().to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bde0cc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "STATUS_TO_KEEP = ['OK FROM REQUESTOK PDF TO TXT', 'OK ALREADY DOWNLOADEDOK PDF TO TXT', 'DOWNLOAD DRIVE - OKOK PDF TO TXT']\n",
    "\n",
    "with open(os.path.join(CHECKPOINT_FOLDER,'whitepaper_recover.pickle'), 'rb') as handle:\n",
    "    final_df_recover = pickle.load(handle)\n",
    "\n",
    "dd = pd.read_csv(os.path.join(RESULTS_FOLDER, '00zz_whitepaper_recover_second_download.csv'), sep = \";\")\n",
    "dd = dd[dd['Status'].isin(STATUS_TO_KEEP)]\n",
    "dd['Recover_action'] = 'FROM SECOND DOWNLOAD'\n",
    "dd['Status'] = 'OK'\n",
    "dd['Recover_Length_txt'] = dd['Final_Length_txt']\n",
    "dd['Recover_Length_txt_clean'] = dd['Final_Length_txt_clean']\n",
    "dd['Recover_Path_txt'] = dd['Final_Path_txt']\n",
    "\n",
    "\n",
    "final_df_recover = pd.concat([final_df_recover, dd])\n",
    "        \n",
    "with open(os.path.join(CHECKPOINT_FOLDER,'whitepaper_recover.pickle'), 'wb') as handle:\n",
    "    pickle.dump(final_df_recover, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7c0882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a54cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755e8609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ICO)",
   "language": "python",
   "name": "ico"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
