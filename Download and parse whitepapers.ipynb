{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75e337e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "import datetime\n",
    "from unidecode import unidecode\n",
    "import pickle\n",
    "import pdfkit\n",
    "import joblib\n",
    "import func_timeout\n",
    "from utils import get_chromedriver, download_from_drive_dropbox, pdf_to_text\n",
    "\n",
    "TESSERACT_PATH=r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'   # used for tesseract OCR. See pdf_to_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de55d101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set folders\n",
    "WHITEPAPER_FOLDER = '.\\\\Whitepaper'\n",
    "ORIGINAL_FOLDER = '.\\\\Whitepaper\\\\Original'\n",
    "CONVERTED_FOLDER = '.\\\\Whitepaper\\\\Converted_to_txt'\n",
    "RECOVERED_FOLDER = '.\\\\Whitepaper\\\\Recovered'\n",
    "CHECKPOINT_FOLDER = '.\\\\Checkpoints'\n",
    "RESULTS_FOLDER = '.\\\\Results'\n",
    "\n",
    "if not os.path.exists(WHITEPAPER_FOLDER):\n",
    "    os.makedirs(WHITEPAPER_FOLDER)\n",
    "if not os.path.exists(ORIGINAL_FOLDER):\n",
    "    os.makedirs(ORIGINAL_FOLDER)\n",
    "if not os.path.exists(CONVERTED_FOLDER):\n",
    "    os.makedirs(CONVERTED_FOLDER)\n",
    "if not os.path.exists(RECOVERED_FOLDER):\n",
    "    os.makedirs(RECOVERED_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eb4d090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>WhitepaperUrl</th>\n",
       "      <th>Status</th>\n",
       "      <th>Error</th>\n",
       "      <th>Path_Original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://icomarks.com/ico/synthetics-ai</td>\n",
       "      <td>https://drive.google.com/file/d/1K7TkqYgCtiarZ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://icomarks.com/ico/777-bingo</td>\n",
       "      <td>https://777.bingo/paper/Whitepaper.EN.pdf</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://icomarks.com/ico/sonic</td>\n",
       "      <td>https://img1.wsimg.com/blobby/go/634ec806-c74c...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://icomarks.com/ico/botchain</td>\n",
       "      <td>botchain.talla.com/whitepaper</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://icomarks.com/ico/eclipse</td>\n",
       "      <td>https://eclipsetoken.io/wp-content/uploads/201...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7508</th>\n",
       "      <td>https://icomarks.com/ico/vanhealthing</td>\n",
       "      <td>https://vanhealthing.com/vanhealthing_wp_en.pdf</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7509</th>\n",
       "      <td>https://icomarks.com/ico/consensus</td>\n",
       "      <td>https://consensus.ai/whitepaper.pdf</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7510</th>\n",
       "      <td>https://icomarks.com/ico/kahnchat</td>\n",
       "      <td>https://www.kahnchat.com/docs/KahnChat-Whitepa...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7511</th>\n",
       "      <td>https://icomarks.com/ico/santiment</td>\n",
       "      <td>https://docs.google.com/document/d/1hHmJQWrPrO...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7512</th>\n",
       "      <td>https://icomarks.com/ico/sophiatx</td>\n",
       "      <td>https://www.sophiatx.com/_data/_custom/SophiaT...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7513 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         url  \\\n",
       "0     https://icomarks.com/ico/synthetics-ai   \n",
       "1         https://icomarks.com/ico/777-bingo   \n",
       "2             https://icomarks.com/ico/sonic   \n",
       "3          https://icomarks.com/ico/botchain   \n",
       "4           https://icomarks.com/ico/eclipse   \n",
       "...                                      ...   \n",
       "7508   https://icomarks.com/ico/vanhealthing   \n",
       "7509      https://icomarks.com/ico/consensus   \n",
       "7510       https://icomarks.com/ico/kahnchat   \n",
       "7511      https://icomarks.com/ico/santiment   \n",
       "7512       https://icomarks.com/ico/sophiatx   \n",
       "\n",
       "                                          WhitepaperUrl Status Error  \\\n",
       "0     https://drive.google.com/file/d/1K7TkqYgCtiarZ...                \n",
       "1             https://777.bingo/paper/Whitepaper.EN.pdf                \n",
       "2     https://img1.wsimg.com/blobby/go/634ec806-c74c...                \n",
       "3                         botchain.talla.com/whitepaper                \n",
       "4     https://eclipsetoken.io/wp-content/uploads/201...                \n",
       "...                                                 ...    ...   ...   \n",
       "7508    https://vanhealthing.com/vanhealthing_wp_en.pdf                \n",
       "7509                https://consensus.ai/whitepaper.pdf                \n",
       "7510  https://www.kahnchat.com/docs/KahnChat-Whitepa...                \n",
       "7511  https://docs.google.com/document/d/1hHmJQWrPrO...                \n",
       "7512  https://www.sophiatx.com/_data/_custom/SophiaT...                \n",
       "\n",
       "     Path_Original  \n",
       "0                   \n",
       "1                   \n",
       "2                   \n",
       "3                   \n",
       "4                   \n",
       "...            ...  \n",
       "7508                \n",
       "7509                \n",
       "7510                \n",
       "7511                \n",
       "7512                \n",
       "\n",
       "[7513 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load whitepaper url\n",
    "download_df = pd.read_csv(os.path.join(RESULTS_FOLDER, '01c_ICOmarks_ico_list_scraped_formatted.csv'), sep = \";\")\n",
    "\n",
    "download_df = download_df[['url', 'WhitepaperUrl']].dropna().reset_index(drop = True)\n",
    "download_df['Status'] = \"\"\n",
    "download_df['Error'] = \"\"\n",
    "download_df['Path_Original'] = \"\"\n",
    "\n",
    "download_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67055fbe",
   "metadata": {},
   "source": [
    "## Download pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5059bf5c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 7513 / 7513  - Total OK: 3046\n",
      "Total elapsed time: 11:06:25\n"
     ]
    }
   ],
   "source": [
    "HEADERS = {\"User-Agent\": \"Chrome/51.0.2704.103\"}\n",
    "URL_ROOT='https://icomarks.com/ico/'    # will be removed from url to create pdf name\n",
    "RELOAD_PDF=True\n",
    "\n",
    "start = timer()\n",
    "OK_count = 0\n",
    "for index, row in download_df.iterrows():\n",
    "    \n",
    "    url = row['WhitepaperUrl']\n",
    "    file_name = os.path.join(ORIGINAL_FOLDER, row['url'].replace(URL_ROOT, '').replace('|', '') + '.pdf')\n",
    "\n",
    "    print('Downloading ' + str(index + 1) + ' / ' + str(len(download_df)) + '  - Total OK: ' + str(OK_count), end = '\\r')\n",
    "    \n",
    "    if not RELOAD_PDF or not os.path.exists(file_name):\n",
    "    \n",
    "        try:\n",
    "            # connect\n",
    "            response = requests.get(url, headers = HEADERS)\n",
    "\n",
    "            # check response and save pdf\n",
    "            if response.status_code == 200:\n",
    "                with open(file_name, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "                download_df.loc[index, 'Status'] = 'OK'\n",
    "                download_df.loc[index, 'Path_Original'] = os.path.join(os.getcwd(), file_name)\n",
    "                OK_count += 1\n",
    "            else:\n",
    "                download_df.loc[index, 'Status'] = response.status_code\n",
    "\n",
    "        except Exception as e:\n",
    "            download_df.loc[index, 'Status'] = 'ERROR'\n",
    "            download_df.loc[index, 'Error'] = e\n",
    "    \n",
    "    else:\n",
    "        download_df.loc[index, 'Status'] = 'OK'\n",
    "        download_df.loc[index, 'Path_Original'] = os.path.join(os.getcwd(), file_name)\n",
    "        OK_count += 1\n",
    "    \n",
    "    # save checkpoint\n",
    "    download_df.to_csv(os.path.join(CHECKPOINT_FOLDER, 'whitepaper_download.csv'), index=False, sep=';')\n",
    "            \n",
    "print('\\nTotal elapsed time:', str(datetime.timedelta(seconds=round(timer()-start))))\n",
    "\n",
    "# save results\n",
    "download_df.to_csv(os.path.join(RESULTS_FOLDER ,'00a_whitepaper_download_original.csv'), index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97523b13",
   "metadata": {},
   "source": [
    "## Check downloaded files and convert to txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9fa6ad5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK       3046\n",
      "ERROR    2712\n",
      "404      1227\n",
      "403       165\n",
      "410        91\n",
      "522        86\n",
      "520        73\n",
      "500        23\n",
      "521        17\n",
      "530        12\n",
      "502        11\n",
      "503        10\n",
      "504         8\n",
      "526         7\n",
      "523         6\n",
      "301         5\n",
      "525         4\n",
      "401         2\n",
      "524         2\n",
      "406         2\n",
      "400         2\n",
      "402         1\n",
      "423         1\n",
      "Name: Status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "download_df = pd.read_csv(os.path.join(RESULTS_FOLDER, '00a_whitepaper_download_original.csv'), sep = \";\")\n",
    "print(download_df['Status'].value_counts())\n",
    "\n",
    "final_df = download_df.copy()\n",
    "final_df = final_df[final_df['Status'] == 'OK'].reset_index(drop = True)\n",
    "final_df['Path_Recovered'] = \"\"\n",
    "final_df['Path_txt'] = \"\"\n",
    "final_df['Status_txt'] = \"\"\n",
    "final_df['Length_txt'] = 0\n",
    "final_df['Length_txt_clean'] = 0\n",
    "final_df['Content_txt'] = \"\"\n",
    "final_df['Metadata'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "016a2874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing 3046 / 3046\n",
      "Total elapsed time: 1:53:33\n"
     ]
    }
   ],
   "source": [
    "# Parse pdf and convert to txt\n",
    "URL_ROOT='https://icomarks.com/ico/'    # will be removed from url to create pdf name\n",
    "RELOAD_PKL=True\n",
    "\n",
    "tot_time=0\n",
    "for index, row in final_df.iterrows():\n",
    "    \n",
    "    file_path = row['Path_Original']\n",
    "    file_path_txt = os.path.join(os.getcwd(), CONVERTED_FOLDER, row['url'].replace(URL_ROOT, '').replace('|', '') + '.txt')\n",
    "    file_path_pkl = os.path.join(os.getcwd(), CONVERTED_FOLDER, row['url'].replace(URL_ROOT, '').replace('|', '') + '.pkl')\n",
    "    \n",
    "    print('Parsing ' + str(index + 1) + ' / ' + str(len(final_df)), end = '\\r')\n",
    "    \n",
    "    if not RELOAD_PKL or not os.path.exists(file_path_pkl):\n",
    "        \n",
    "        # pdf to txt\n",
    "        start = timer()\n",
    "        txt, meta, parsed_pdf = pdf_to_text(file_path=file_path, tesseract_path=TESSERACT_PATH, lang='eng')\n",
    "        status=parsed_pdf['status']\n",
    "        eval_time=datetime.timedelta(seconds=round(timer()-start)).total_seconds()\n",
    "        \n",
    "        # save .pkl\n",
    "        joblib.dump({'txt': txt, 'meta': meta, 'status': status, 'eval_time': eval_time}, file_path_pkl)\n",
    "    \n",
    "    else:\n",
    "        rr=joblib.load(file_path_pkl)\n",
    "        txt=rr['txt']\n",
    "        meta=rr['meta']\n",
    "        status=rr['status']\n",
    "        eval_time=rr['eval_time'] \n",
    "        \n",
    "    final_df.loc[index, 'Status_txt'] = status\n",
    "    final_df.loc[index, 'Length_txt'] = len(txt) if txt is not None else 0    # case of pdf saved as image\n",
    "    final_df.loc[index, 'Length_txt_clean'] = len(txt.replace('\\n','').replace(' ', '')) if txt is not None else 0    # clear whitespace to filter empty files\n",
    "    final_df.at[index, 'Content_txt'] = meta['Content-Type']\n",
    "    final_df.loc[index, 'Metadata'] = [meta]\n",
    "    tot_time+=eval_time\n",
    "        \n",
    "    # save txt\n",
    "    if txt is not None:\n",
    "        final_df.loc[index, 'Path_txt'] = file_path_txt\n",
    "        with open(file_path_txt, 'w') as f:\n",
    "            f.write(unidecode(txt))\n",
    "    \n",
    "    # save checkpoint\n",
    "    if index % 300 == 0 or index == (len(final_df) - 1):\n",
    "        final_df.drop(columns='Metadata').to_csv(os.path.join(CHECKPOINT_FOLDER, 'whitepaper_parsing.csv'), index=False, sep=';')\n",
    "        with open(os.path.join(CHECKPOINT_FOLDER, 'whitepaper_parsing.pickle'), 'wb') as handle:\n",
    "                    pickle.dump(final_df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "print('\\nTotal elapsed time:', str(datetime.timedelta(seconds=round(tot_time))))\n",
    "\n",
    "# save results\n",
    "final_df.drop(columns='Metadata').to_csv(os.path.join(RESULTS_FOLDER, '00b_whitepaper_parsing.csv'), index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6463f9c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Status --\n",
      "\n",
      "OK    3046\n",
      "Name: Status, dtype: int64\n",
      "\n",
      "\n",
      "-- Text length --\n",
      "\n",
      "          Length_txt  Length_txt_clean\n",
      "count    3046.000000       3046.000000\n",
      "mean    22247.210112      17159.006894\n",
      "std     35160.519442      27759.354711\n",
      "min         0.000000          0.000000\n",
      "10%        71.000000         33.000000\n",
      "15%        82.750000         44.000000\n",
      "20%       132.000000         69.000000\n",
      "25%       320.250000        106.000000\n",
      "30%       507.500000        286.000000\n",
      "40%      2322.000000       1256.000000\n",
      "50%      6009.500000       3623.500000\n",
      "75%     34120.750000      26762.750000\n",
      "95%     88420.000000      71595.250000\n",
      "max    485969.000000     411334.000000\n",
      "\n",
      "\n",
      "-- Pdf content --\n",
      "\n",
      "- Single values in \"Content_txt\" (2938):\n",
      "text/html; charset=UTF-8                       1185\n",
      "application/pdf                                1157\n",
      "text/html; charset=ISO-8859-1                   387\n",
      "application/xhtml+xml; charset=UTF-8            133\n",
      "application/xhtml+xml; charset=ISO-8859-1        36\n",
      "application/octet-stream                         12\n",
      "text/plain; charset=ISO-8859-1                   12\n",
      "application/xhtml+xml; charset=windows-1252       5\n",
      "text/html; charset=windows-1252                   4\n",
      "image/png                                         2\n",
      "text/html; charset=EUC-KR                         2\n",
      "text/plain; charset=windows-1252                  1\n",
      "application/xhtml+xml; charset=GB2312             1\n",
      "text/html; charset=Shift_JIS                      1\n",
      "Name: Content_txt, dtype: int64\n",
      "\n",
      "- Multiple values in \"Content_txt\" (108) first column is the number of multiple values in list:\n",
      "2      29\n",
      "9       7\n",
      "3       6\n",
      "11      5\n",
      "4       5\n",
      "19      5\n",
      "8       3\n",
      "18      3\n",
      "26      3\n",
      "21      3\n",
      "23      2\n",
      "15      2\n",
      "41      2\n",
      "42      2\n",
      "14      2\n",
      "31      2\n",
      "5       2\n",
      "24      2\n",
      "20      2\n",
      "7       2\n",
      "47      1\n",
      "35      1\n",
      "104     1\n",
      "80      1\n",
      "326     1\n",
      "165     1\n",
      "52      1\n",
      "12      1\n",
      "13      1\n",
      "10      1\n",
      "89      1\n",
      "37      1\n",
      "6       1\n",
      "61      1\n",
      "36      1\n",
      "201     1\n",
      "27      1\n",
      "387     1\n",
      "38      1\n",
      "Name: aa, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# stats\n",
    "with open(os.path.join(CHECKPOINT_FOLDER, 'whitepaper_parsing.pickle'), 'rb') as handle:\n",
    "    final_df = pickle.load(handle)\n",
    "\n",
    "# status\n",
    "print('-- Status --\\n')\n",
    "print(final_df['Status'].value_counts())\n",
    "# text length\n",
    "print('\\n\\n-- Text length --\\n')\n",
    "print(final_df[['Length_txt', 'Length_txt_clean']].describe(percentiles = [.5, .10, .15, .2, .25, .3, .4, .75, .95]))\n",
    "# pdf content\n",
    "cnt = final_df['Content_txt'].values\n",
    "cnt_single = [x for x in cnt if type(x) == str]\n",
    "cnt_multiple = [x for x in cnt if type(x) != str]\n",
    "print('\\n\\n-- Pdf content --')\n",
    "print('\\n- Single values in \"Content_txt\" (' + str(len(cnt_single)) + '):')\n",
    "print(pd.DataFrame({'Content_txt': cnt_single})['Content_txt'].value_counts())\n",
    "print('\\n- Multiple values in \"Content_txt\" (' + str(len(cnt_multiple)) +') first column is the number of multiple values in list:')\n",
    "print(pd.DataFrame({'aa': [len(x) for x in cnt_multiple]})['aa'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7330fcc8",
   "metadata": {},
   "source": [
    "## Try to recover empty pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a87cd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "Length_txt_clean_thrsh = 4000     # threshold for maximum non-empty characters in parsed txt file\n",
    "\n",
    "\n",
    "with open(os.path.join(CHECKPOINT_FOLDER, 'whitepaper_parsing.pickle'), 'rb') as handle:\n",
    "    final_df_recover = pickle.load(handle)\n",
    "final_df_recover['Recover_action'] = \"SKIP\"\n",
    "final_df_recover['Recover_Length_txt'] = -1\n",
    "final_df_recover['Recover_Length_txt_clean'] = -1\n",
    "final_df_recover['Recover_Path_txt'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47a1e823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recovering 3046 / 3046 (santiment)                                                                      \n",
      "Total elapsed time: 2:40:53\n"
     ]
    }
   ],
   "source": [
    "URL_ROOT='https://icomarks.com/ico/'    # will be removed from url to create pdf name\n",
    "PATH_TO_WHHTMLTOPDF = r'C:\\Program Files\\wkhtmltopdf\\bin\\wkhtmltopdf.exe' # define path to wkhtmltopdf.exe, see https://python-bloggers.com/2022/06/convert-html-to-pdf-using-python/\n",
    "CHROMEDRIVER_PATH = r\"C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\ICOs\\WebDriver\\chromedriver\"\n",
    "TEMP_DOWNLOAD_FOLDER = \"C:\\\\Users\\\\Alessandro Bitetto\\\\Downloads\\\\UniPV\\\\ICOs\\\\temp_download\"\n",
    "ID_TO_SKIP = ['494', '1369', '2527', '2548', '4100', '6138']\n",
    "RELOAD_PDF = True\n",
    "RELOAD_PKL = True\n",
    "\n",
    "# Point pdfkit configuration to wkhtmltopdf.exe\n",
    "config = pdfkit.configuration(wkhtmltopdf=PATH_TO_WHHTMLTOPDF)\n",
    "start = timer()\n",
    "for index, row in final_df_recover.iterrows():\n",
    "\n",
    "    short_name=row['url'].replace(URL_ROOT, '').replace('|', '')\n",
    "    \n",
    "    print('Recovering ' + str(index + 1) + ' / ' + str(len(final_df_recover)) + ' (' + short_name + ')' +' '*30, end = '\\r')\n",
    "\n",
    "    file_path_pdf = os.path.join(os.getcwd(), RECOVERED_FOLDER, short_name + '.pdf')\n",
    "    file_path_txt = os.path.join(os.getcwd(), RECOVERED_FOLDER, short_name + '.txt')\n",
    "    file_path_pkl = os.path.join(os.getcwd(), RECOVERED_FOLDER, short_name + '.pkl')\n",
    "    original_len_txt = row['Length_txt']\n",
    "    original_len_txt_clean = row['Length_txt_clean']\n",
    "    url = row['WhitepaperUrl']\n",
    "    \n",
    "    # check if txt length is below threshold\n",
    "    if row['Length_txt_clean'] <= Length_txt_clean_thrsh:\n",
    "        \n",
    "        if short_name in ID_TO_SKIP:\n",
    "            pass\n",
    "        \n",
    "        ##### download pdf from google.drive or dropbox\n",
    "\n",
    "        elif any(x in url for x in ['google', 'goo.gl', 'dropbox']):\n",
    "\n",
    "            source = 'drive' if any(x in url for x in ['google', 'goo.gl']) else 'dropbox'\n",
    "\n",
    "            if not RELOAD_PDF or not os.path.exists(file_path_pdf):\n",
    "            \n",
    "                out = download_from_drive_dropbox(chromedriver_path=CHROMEDRIVER_PATH, download_url=url,\n",
    "                                                  download_folder=TEMP_DOWNLOAD_FOLDER, temp_folder=TEMP_DOWNLOAD_FOLDER,\n",
    "                                                  pdf_name=short_name + '.pdf',\n",
    "                                                  move_folder=os.path.join(os.getcwd(), RECOVERED_FOLDER), source=source)\n",
    "            else:\n",
    "                out=\"ok\"\n",
    "\n",
    "            if out == \"ok\":\n",
    "\n",
    "                # pdf to txt\n",
    "                if not RELOAD_PKL or not os.path.exists(file_path_pkl):\n",
    "        \n",
    "                    # pdf to txt\n",
    "                    start_t = timer()\n",
    "                    txt, meta, parsed_pdf = pdf_to_text(file_path=file_path_pdf, tesseract_path=TESSERACT_PATH, lang='eng')\n",
    "                    status=parsed_pdf['status']\n",
    "                    eval_time=datetime.timedelta(seconds=round(timer()-start_t)).total_seconds()\n",
    "\n",
    "                    # save .pkl\n",
    "                    joblib.dump({'txt': txt, 'meta': meta, 'status': status, 'eval_time': eval_time}, file_path_pkl)\n",
    "\n",
    "                else:\n",
    "                    rr=joblib.load(file_path_pkl)\n",
    "                    txt=rr['txt']\n",
    "                    meta=rr['meta']\n",
    "\n",
    "                len_txt_clean = len(txt.replace('\\n','').replace(' ', '')) if txt is not None else 0 \n",
    "\n",
    "                # update files\n",
    "                if len_txt_clean > original_len_txt_clean:\n",
    "\n",
    "                    final_df_recover.loc[index, 'Recover_action'] = 'DOWNLOAD ' + source.upper() + ' - OK'\n",
    "                    final_df_recover.loc[index, 'Path_Recovered'] = file_path_pdf\n",
    "                    final_df_recover.loc[index, 'Recover_Path_txt'] = file_path_txt\n",
    "                    final_df_recover.loc[index, 'Recover_Length_txt'] = len(txt)\n",
    "                    final_df_recover.loc[index, 'Recover_Length_txt_clean'] = len_txt_clean\n",
    "                    final_df_recover.at[index, 'Content_txt'] = meta['Content-Type']\n",
    "                    final_df_recover.loc[index, 'Metadata'] = [meta]\n",
    "\n",
    "                    # save txt\n",
    "                    with open(file_path_txt, 'w') as f:\n",
    "                        f.write(unidecode(txt))\n",
    "\n",
    "            else:\n",
    "                final_df_recover.loc[index, 'Recover_action'] = 'DOWNLOAD ' + source.upper() + ' - ' + out\n",
    "        \n",
    "        \n",
    "        ##### try to download pdf from html page   e.g. https://www.quasa.io/white-paper\n",
    "        \n",
    "        else:\n",
    "\n",
    "            try:\n",
    "                # Convert Webpage to PDF\n",
    "                if not RELOAD_PDF or not os.path.exists(file_path_pdf):\n",
    "                    def download_web(url, output_path):\n",
    "                        pdfkit.from_url(url, output_path=output_path, configuration=config)\n",
    "                    def run_function(f, max_wait):\n",
    "                        try:\n",
    "                            func_timeout.func_timeout(max_wait, download_web, args=(url, file_path_pdf))\n",
    "                            return 'ok'\n",
    "                        except func_timeout.FunctionTimedOut:\n",
    "                            pass\n",
    "                        return 'timeout'\n",
    "                    out = run_function(download_web, 80)    # stop running after 60*2 seconds\n",
    "                    if out == 'timeout':\n",
    "                        with open(file_path_pdf, 'w') as outfile:     # save empty pdf so speed up when RELOAD_PDF=True\n",
    "                            outfile.write(\"\")\n",
    "\n",
    "                # pdf to txt\n",
    "                if not RELOAD_PKL or not os.path.exists(file_path_pkl):\n",
    "        \n",
    "                    # pdf to txt\n",
    "                    start_t = timer()\n",
    "                    txt, meta, parsed_pdf = pdf_to_text(file_path=file_path_pdf, tesseract_path=TESSERACT_PATH, lang='eng')\n",
    "                    status=parsed_pdf['status']\n",
    "                    eval_time=datetime.timedelta(seconds=round(timer()-start_t)).total_seconds()\n",
    "\n",
    "                    # save .pkl\n",
    "                    joblib.dump({'txt': txt, 'meta': meta, 'status': status, 'eval_time': eval_time}, file_path_pkl)\n",
    "\n",
    "                else:\n",
    "                    rr=joblib.load(file_path_pkl)\n",
    "                    txt=rr['txt']\n",
    "\n",
    "                len_txt_clean = len(txt.replace('\\n','').replace(' ', '')) if txt is not None else 0 \n",
    "\n",
    "                # update files\n",
    "                if len_txt_clean > original_len_txt_clean:\n",
    "\n",
    "                    final_df_recover.loc[index, 'Recover_action'] = \"CONVERT FROM HTML\"\n",
    "                    final_df_recover.loc[index, 'Path_Recovered'] = file_path_pdf\n",
    "                    final_df_recover.loc[index, 'Recover_Path_txt'] = file_path_txt\n",
    "                    final_df_recover.loc[index, 'Recover_Length_txt'] = len(txt)\n",
    "                    final_df_recover.loc[index, 'Recover_Length_txt_clean'] = len_txt_clean\n",
    "                    final_df_recover.at[index, 'Content_txt'] = meta['Content-Type']\n",
    "                    final_df_recover.loc[index, 'Metadata'] = [meta]\n",
    "\n",
    "                    # save txt\n",
    "                    with open(file_path_txt, 'w') as f:\n",
    "                        f.write(unidecode(txt))\n",
    "            except:\n",
    "                final_df_recover.loc[index, 'Recover_action'] = \"CONVERT FROM HTML - FAILED\"\n",
    "\n",
    "    else:\n",
    "        final_df_recover.loc[index, 'Recover_action'] = \"KEEP ORIGINAL\"\n",
    "        \n",
    "    # save checkpoint\n",
    "    if index % 300 == 0 or index == (len(final_df_recover) - 1):\n",
    "        final_df_recover.drop(columns='Metadata').to_csv(os.path.join(CHECKPOINT_FOLDER, 'whitepaper_recover.csv'), index=False, sep=';')\n",
    "        with open(os.path.join(CHECKPOINT_FOLDER,'whitepaper_recover.pickle'), 'wb') as handle:\n",
    "                    pickle.dump(final_df_recover, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "print('\\nTotal elapsed time:', str(datetime.timedelta(seconds=round(timer()-start))))\n",
    "\n",
    "# save results\n",
    "final_df_recover.drop(columns='Metadata').to_csv(os.path.join(RESULTS_FOLDER, '00c_whitepaper_recover.csv'), index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fc3565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recover actions stats\n",
    "with open('./Checkpoints/whitepaper_recover.pickle', 'rb') as handle:\n",
    "    final_df_recover = pickle.load(handle)\n",
    "\n",
    "print('-- Recover actions --\\n')\n",
    "print(final_df_recover['Recover_action'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de87b0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Recover actions --\n",
      "\n",
      "KEEP ORIGINAL                            1049\n",
      "SKIP                                      345\n",
      "CONVERT FROM HTML                         249\n",
      "DOWNLOAD DRIVE - OK                       149\n",
      "CONVERT FROM HTML - FAILED                124\n",
      "DOWNLOAD DRIVE - page not available       114\n",
      "DOWNLOAD DROPBOX - page not available      17\n",
      "DOWNLOAD DROPBOX - OK                       9\n",
      "DOWNLOAD DRIVE - out of time                2\n",
      "Name: Recover_action, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# recover actions stats\n",
    "with open('./Checkpoints/whitepaper_recover.pickle', 'rb') as handle:\n",
    "    final_df_recover = pickle.load(handle)\n",
    "\n",
    "print('-- Recover actions --\\n')\n",
    "print(final_df_recover['Recover_action'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25cb97bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total available whitepapers: 1456\n"
     ]
    }
   ],
   "source": [
    "# set final path for txt files\n",
    "recover_to_keep = ['CONVERT FROM HTML', 'DOWNLOAD DRIVE - OK', 'DOWNLOAD DROPBOX - OK']\n",
    "\n",
    "final_df_recover['Final_Path_txt'] = np.where(final_df_recover['Recover_action'] == 'KEEP ORIGINAL', final_df_recover['Path_txt'], '')\n",
    "final_df_recover['Final_Path_txt'] = np.where(final_df_recover['Recover_action'].isin(recover_to_keep), final_df_recover['Recover_Path_txt'], final_df_recover['Final_Path_txt'])\n",
    "final_df_recover['Final_Length_txt'] = np.where(final_df_recover['Recover_action'] == 'KEEP ORIGINAL', final_df_recover['Length_txt'], '')\n",
    "final_df_recover['Final_Length_txt'] = np.where(final_df_recover['Recover_action'].isin(recover_to_keep), final_df_recover['Recover_Length_txt'], final_df_recover['Final_Length_txt'])\n",
    "final_df_recover['Final_Length_txt_clean'] = np.where(final_df_recover['Recover_action'] == 'KEEP ORIGINAL', final_df_recover['Length_txt_clean'], '')\n",
    "final_df_recover['Final_Length_txt_clean'] = np.where(final_df_recover['Recover_action'].isin(recover_to_keep), final_df_recover['Recover_Length_txt_clean'], final_df_recover['Final_Length_txt_clean'])\n",
    "\n",
    "\n",
    "final_df_recover.drop(columns='Metadata').to_csv('./Results/00d_whitepaper_final.csv', index=False, sep=';')\n",
    "\n",
    "print('Total available whitepapers:', sum(final_df_recover['Final_Path_txt'] != ''))\n",
    "with open('./Checkpoints/whitepaper_final.pickle', 'wb') as handle:\n",
    "    pickle.dump(final_df_recover, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8cfa466",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link_white_paper</th>\n",
       "      <th>Status</th>\n",
       "      <th>Error</th>\n",
       "      <th>Path_Original</th>\n",
       "      <th>Path_Recovered</th>\n",
       "      <th>Path_txt</th>\n",
       "      <th>Status_txt</th>\n",
       "      <th>Length_txt</th>\n",
       "      <th>Length_txt_clean</th>\n",
       "      <th>Content_txt</th>\n",
       "      <th>Metadata</th>\n",
       "      <th>Recover_action</th>\n",
       "      <th>Recover_Length_txt</th>\n",
       "      <th>Recover_Length_txt_clean</th>\n",
       "      <th>Recover_Path_txt</th>\n",
       "      <th>Final_Path_txt</th>\n",
       "      <th>Final_Length_txt</th>\n",
       "      <th>Final_Length_txt_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>https://www.tycoon.io/whitepaper.pdf</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td></td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td>200</td>\n",
       "      <td>58501</td>\n",
       "      <td>48450</td>\n",
       "      <td>application/pdf</td>\n",
       "      <td>{'Content-Type': 'application/pdf', 'Last-Modi...</td>\n",
       "      <td>KEEP ORIGINAL</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td>58501</td>\n",
       "      <td>48450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>https://mindsync.ai/docs/whitepaper.pdf</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td></td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td>200</td>\n",
       "      <td>73801</td>\n",
       "      <td>61180</td>\n",
       "      <td>application/pdf</td>\n",
       "      <td>{'Content-Type': 'application/pdf', 'Creation-...</td>\n",
       "      <td>KEEP ORIGINAL</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td>73801</td>\n",
       "      <td>61180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>https://lohncontrol.com/down/LOHN-white-paper-...</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td></td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td>200</td>\n",
       "      <td>74525</td>\n",
       "      <td>59500</td>\n",
       "      <td>application/pdf</td>\n",
       "      <td>{'Author': 'Vali', 'Content-Type': 'applicatio...</td>\n",
       "      <td>KEEP ORIGINAL</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td>74525</td>\n",
       "      <td>59500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>https://emanate.live/pdf/mn8-whitepaper-v12.pdf</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td>200</td>\n",
       "      <td>245</td>\n",
       "      <td>166</td>\n",
       "      <td>application/pdf</td>\n",
       "      <td>{'Content-Type': 'application/pdf', 'Creation-...</td>\n",
       "      <td>CONVERT FROM HTML</td>\n",
       "      <td>365</td>\n",
       "      <td>292</td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td>365</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>https://hashbon.com/info/whitepaper_eng.pdf</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td></td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td>200</td>\n",
       "      <td>11197</td>\n",
       "      <td>9114</td>\n",
       "      <td>application/pdf</td>\n",
       "      <td>{'Content-Type': 'application/pdf', 'Creation-...</td>\n",
       "      <td>KEEP ORIGINAL</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td>11197</td>\n",
       "      <td>9114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                   link_white_paper Status Error  \\\n",
       "0   2               https://www.tycoon.io/whitepaper.pdf     OK   NaN   \n",
       "1   3            https://mindsync.ai/docs/whitepaper.pdf     OK   NaN   \n",
       "2   5  https://lohncontrol.com/down/LOHN-white-paper-...     OK   NaN   \n",
       "3   6    https://emanate.live/pdf/mn8-whitepaper-v12.pdf     OK   NaN   \n",
       "4   7        https://hashbon.com/info/whitepaper_eng.pdf     OK   NaN   \n",
       "\n",
       "                                       Path_Original  \\\n",
       "0  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...   \n",
       "1  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...   \n",
       "2  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...   \n",
       "3  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...   \n",
       "4  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...   \n",
       "\n",
       "                                      Path_Recovered  \\\n",
       "0                                                      \n",
       "1                                                      \n",
       "2                                                      \n",
       "3  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...   \n",
       "4                                                      \n",
       "\n",
       "                                            Path_txt Status_txt  Length_txt  \\\n",
       "0  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...        200       58501   \n",
       "1  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...        200       73801   \n",
       "2  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...        200       74525   \n",
       "3  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...        200         245   \n",
       "4  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...        200       11197   \n",
       "\n",
       "   Length_txt_clean      Content_txt  \\\n",
       "0             48450  application/pdf   \n",
       "1             61180  application/pdf   \n",
       "2             59500  application/pdf   \n",
       "3               166  application/pdf   \n",
       "4              9114  application/pdf   \n",
       "\n",
       "                                            Metadata     Recover_action  \\\n",
       "0  {'Content-Type': 'application/pdf', 'Last-Modi...      KEEP ORIGINAL   \n",
       "1  {'Content-Type': 'application/pdf', 'Creation-...      KEEP ORIGINAL   \n",
       "2  {'Author': 'Vali', 'Content-Type': 'applicatio...      KEEP ORIGINAL   \n",
       "3  {'Content-Type': 'application/pdf', 'Creation-...  CONVERT FROM HTML   \n",
       "4  {'Content-Type': 'application/pdf', 'Creation-...      KEEP ORIGINAL   \n",
       "\n",
       "   Recover_Length_txt  Recover_Length_txt_clean  \\\n",
       "0                  -1                        -1   \n",
       "1                  -1                        -1   \n",
       "2                  -1                        -1   \n",
       "3                 365                       292   \n",
       "4                  -1                        -1   \n",
       "\n",
       "                                    Recover_Path_txt  \\\n",
       "0                                                      \n",
       "1                                                      \n",
       "2                                                      \n",
       "3  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...   \n",
       "4                                                      \n",
       "\n",
       "                                      Final_Path_txt Final_Length_txt  \\\n",
       "0  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...            58501   \n",
       "1  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...            73801   \n",
       "2  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...            74525   \n",
       "3  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...              365   \n",
       "4  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...            11197   \n",
       "\n",
       "  Final_Length_txt_clean  \n",
       "0                  48450  \n",
       "1                  61180  \n",
       "2                  59500  \n",
       "3                    292  \n",
       "4                   9114  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_recover.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ICO)",
   "language": "python",
   "name": "ico"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
