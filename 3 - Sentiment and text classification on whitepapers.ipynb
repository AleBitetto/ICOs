{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "185a24c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Alessandro\n",
      "[nltk_data]     Bitetto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Alessandro\n",
      "[nltk_data]     Bitetto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Alessandro\n",
      "[nltk_data]     Bitetto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "import datetime\n",
    "import pickle\n",
    "import joblib\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import re\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "from utils import sentence_classification\n",
    "# import logging\n",
    "\n",
    "CACHE_DIR = 'D:/huggingface_cache/'    # cache directory for huggingface models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4312abbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set folders\n",
    "CHECKPOINT_FOLDER = '.\\\\Checkpoints'\n",
    "RESULTS_FOLDER = '.\\\\Results'\n",
    "SENTIMENT_FOLDER = '.\\\\Checkpoints\\\\Sentiment'\n",
    "\n",
    "if not os.path.exists(CHECKPOINT_FOLDER):\n",
    "    os.makedirs(CHECKPOINT_FOLDER)\n",
    "if not os.path.exists(RESULTS_FOLDER):\n",
    "    os.makedirs(RESULTS_FOLDER)\n",
    "if not os.path.exists(SENTIMENT_FOLDER):\n",
    "    os.makedirs(SENTIMENT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3563fdc7",
   "metadata": {},
   "source": [
    "## Load text files, clean and do lemmatisation\n",
    "\n",
    "ICOs with \"FundRaisedUSD\" will be kept anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5227aa2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1867 / 1867\n",
      "Total elapsed time: 0:00:01\n"
     ]
    }
   ],
   "source": [
    "# load text\n",
    "LENGTH_TXT_CLEAN_THRSH = 4000     # threshold for maximum non-empty characters in parsed txt file\n",
    "\n",
    "# available \"FundRaisedUSD\"\n",
    "formatted_df=pd.read_pickle(os.path.join(CHECKPOINT_FOLDER, 'formatted_df.pkl'))\n",
    "avail_fund_url=formatted_df[~formatted_df['FundRaisedUSD'].isna()]['url'].values\n",
    "\n",
    "with open(os.path.join(CHECKPOINT_FOLDER, 'whitepaper_final.pickle'), 'rb') as handle:\n",
    "    final_df = pickle.load(handle)\n",
    "\n",
    "df_text = (final_df[(final_df['Final_Length_txt_clean'] >= LENGTH_TXT_CLEAN_THRSH) | (final_df['url'].isin(avail_fund_url))]\n",
    "           [['url', 'Final_Length_txt', 'Final_Length_txt_clean', 'Final_Path_txt']])\n",
    "df_text = df_text[df_text['Final_Path_txt'] != ''].reset_index(drop = True)\n",
    "df_text['text'] = ''\n",
    "\n",
    "start = timer()\n",
    "for index, row in df_text.iterrows():\n",
    "    \n",
    "    print('Reading ' + str(index + 1) + ' / ' + str(len(df_text)), end = '\\r')\n",
    "    \n",
    "    with open(row['Final_Path_txt']) as f:\n",
    "        df_text.loc[index, 'text'] = f.read()\n",
    "print('\\nTotal elapsed time:', str(datetime.timedelta(seconds=round(timer()-start))))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60e19f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Cleaning...\n",
      "- Lemmatisation...\n",
      "\n",
      "- 1 rows removed because of no word left\n",
      "\n",
      "Total elapsed time: 0:01:25\n",
      "\n",
      "Data saved in .\\Checkpoints\\df_text.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>Final_Length_txt</th>\n",
       "      <th>Final_Length_txt_clean</th>\n",
       "      <th>Final_Path_txt</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://icomarks.com/ico/the-mill-of-blood</td>\n",
       "      <td>23152</td>\n",
       "      <td>8655</td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>the mill blood mill blood about white paper ic...</td>\n",
       "      <td>975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://icomarks.com/ico/moonlight</td>\n",
       "      <td>64263</td>\n",
       "      <td>53667</td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>moonlight white paper important notice please ...</td>\n",
       "      <td>6166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://icomarks.com/ico/digithoth</td>\n",
       "      <td>22151</td>\n",
       "      <td>5950</td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTopsoils...</td>\n",
       "      <td>topsoil northwest choice turf sod topsoil bark...</td>\n",
       "      <td>825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://icomarks.com/ico/tourcom-blockchain</td>\n",
       "      <td>50350</td>\n",
       "      <td>40533</td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>powerpoint peurejenteisyeon tourcom blockchain...</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://icomarks.com/ico/migland</td>\n",
       "      <td>33630</td>\n",
       "      <td>27525</td>\n",
       "      <td>C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>powerpoint peurejenteisyeon mig whitepaper enj...</td>\n",
       "      <td>3183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           url Final_Length_txt  \\\n",
       "0   https://icomarks.com/ico/the-mill-of-blood            23152   \n",
       "1           https://icomarks.com/ico/moonlight            64263   \n",
       "2           https://icomarks.com/ico/digithoth            22151   \n",
       "3  https://icomarks.com/ico/tourcom-blockchain            50350   \n",
       "4             https://icomarks.com/ico/migland            33630   \n",
       "\n",
       "   Final_Length_txt_clean                                     Final_Path_txt  \\\n",
       "0                    8655  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...   \n",
       "1                   53667  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...   \n",
       "2                    5950  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...   \n",
       "3                   40533  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...   \n",
       "4                   27525  C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\IC...   \n",
       "\n",
       "                                                text  \\\n",
       "0  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "1  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "2  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTopsoils...   \n",
       "3  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "4  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "\n",
       "                                          text_clean  word_count  \n",
       "0  the mill blood mill blood about white paper ic...         975  \n",
       "1  moonlight white paper important notice please ...        6166  \n",
       "2  topsoil northwest choice turf sod topsoil bark...         825  \n",
       "3  powerpoint peurejenteisyeon tourcom blockchain...        4759  \n",
       "4  powerpoint peurejenteisyeon mig whitepaper enj...        3183  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean and lemmatisation\n",
    "# https://github.com/Briiick/NLP-disaster-tweets/blob/main/notebooks/3-heavy-cleaning-BERT.ipynb\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\"nan\"])\n",
    "\n",
    "def heavy_text_clean(x):\n",
    "    # first we lowercase everything\n",
    "    x = x.lower()\n",
    "    x = ' '.join([word for word in x.split(' ') if word not in stopwords])\n",
    "    # remove unicode characters\n",
    "    x = x.encode('ascii', 'ignore').decode()\n",
    "    x = re.sub(r'https*\\S+', ' ', x)\n",
    "    x = re.sub(r'http*\\S+', ' ', x)\n",
    "    # then use regex to remove @ symbols and hashtags\n",
    "    x = re.sub(r'@\\S', '', x)\n",
    "    x = re.sub(r'#\\S+', ' ', x)\n",
    "    x = re.sub(r'\\'\\w+', '', x)\n",
    "    x = re.sub('[%s]' % re.escape(string.punctuation), ' ', x)\n",
    "    x = re.sub(r'\\w*\\d+\\w*', '', x)\n",
    "    x = re.sub(r'\\s{2,}', ' ', x)\n",
    "    x = re.sub(r'\\s[^\\w\\s]\\s', '', x)\n",
    "    # remove single letters and numbers surrounded by space\n",
    "    x = re.sub(r'\\s[a-z]\\s|\\s[0-9]\\s', ' ', x)\n",
    "    return x\n",
    "\n",
    "def lemmatise(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "start = timer()\n",
    "# apply cleaning\n",
    "print('- Cleaning...')\n",
    "df_text['text_clean'] = df_text['text'].apply(heavy_text_clean)\n",
    "\n",
    "# apply lemmatisation\n",
    "print('- Lemmatisation...')\n",
    "df_text['text_clean'] = df_text['text_clean'].apply(lemmatise)\n",
    "df_text['word_count'] = df_text['text_clean'].str.split().apply(len)\n",
    "\n",
    "# remove text with no words\n",
    "check=df_text[df_text['word_count'] == 0]\n",
    "if len(check) > 0:\n",
    "    print(f'\\n- {len(check)} rows removed because of no word left')\n",
    "    df_text=df_text[df_text['word_count'] > 0]\n",
    "\n",
    "print('\\nTotal elapsed time:', str(datetime.timedelta(seconds=round(timer()-start))))\n",
    "\n",
    "pkl_path=os.path.join(CHECKPOINT_FOLDER, 'df_text.pkl')\n",
    "joblib.dump(df_text, pkl_path, compress=('lzma', 3))\n",
    "print(f'\\nData saved in {pkl_path}')\n",
    "\n",
    "df_text.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66faf32",
   "metadata": {},
   "source": [
    "##  Perform sentiment analysis and text classification (pre-trained models)\n",
    "### with HuggingFace API\n",
    "\n",
    "https://huggingface.co/docs/api-inference/detailed_parameters\n",
    "\n",
    "https://towardsdatascience.com/how-to-apply-transformers-to-any-length-of-text-a5601410af7f\n",
    "\n",
    "https://towardsdatascience.com/does-bert-need-clean-data-part-2-classification-d29adf9f745a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11b13f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text=joblib.load(os.path.join(CHECKPOINT_FOLDER, 'df_text.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "518838ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "##################################################################################################\n",
      "#                                    yiyanghkust/finbert-tone                                    #\n",
      "################################################################################################## \n",
      "\n",
      "\n",
      "Prediction classes:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Class\n",
       "0   Neutral\n",
       "1  Positive\n",
       "2  Negative"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum tokens allowed: 512\n",
      "\n",
      "Special Tokens:\n",
      "[PAD]: 0 \n",
      "[CLS]: 3 \n",
      "[SEP]: 4 \n",
      "[UNK]: 2\n",
      "\n",
      "\n",
      "-- Split sentences into chunks:\n",
      "\n",
      "Reloaded (evaluated in 0:05:23)\n",
      "\n",
      "Total sentences: 1866\n",
      "Total chunked sentences: 22852\n",
      "\n",
      "\n",
      "-- Query from API:\n",
      "\n",
      "Querying batch (50 rows) 458 / 458  last interaction: 19/02/2023 11:57:07  - total failed batch: 0 (0 rows)\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>OK</th>\n",
       "      <td>22852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Status\n",
       "OK   22852"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done in  5:19:40\n",
      "Data saved in .\\Checkpoints\\00_sentence_query_yiyanghkust_finbert-tone.pkl\n",
      "\n",
      "\n",
      "##############################################################################################################\n",
      "#                                    yiyanghkust/finbert-esg-9-categories                                    #\n",
      "############################################################################################################## \n",
      "\n",
      "\n",
      "Prediction classes:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Climate Change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Natural Capital</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pollution &amp; Waste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Human Capital</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Product Liability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Community Relations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Corporate Governance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Business Ethics &amp; Values</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Non-ESG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Class\n",
       "0            Climate Change\n",
       "1           Natural Capital\n",
       "2         Pollution & Waste\n",
       "3             Human Capital\n",
       "4         Product Liability\n",
       "5       Community Relations\n",
       "6      Corporate Governance\n",
       "7  Business Ethics & Values\n",
       "8                   Non-ESG"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum tokens allowed: 512\n",
      "\n",
      "Special Tokens:\n",
      "[PAD]: 0 \n",
      "[CLS]: 3 \n",
      "[SEP]: 4 \n",
      "[UNK]: 2\n",
      "\n",
      "\n",
      "-- Split sentences into chunks:\n",
      "\n",
      "Reloaded (evaluated in 0:05:26)\n",
      "\n",
      "Total sentences: 1866\n",
      "Total chunked sentences: 22852\n",
      "\n",
      "\n",
      "-- Query from API:\n",
      "\n",
      "Querying batch (50 rows) 458 / 458  last interaction: 19/02/2023 11:59:50  - total failed batch: 0 (0 rows)\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>OK</th>\n",
       "      <td>22852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Status\n",
       "OK   22852"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done in  5:59:45\n",
      "Data saved in .\\Checkpoints\\00_sentence_query_yiyanghkust_finbert-esg-9-categories.pkl\n",
      "\n",
      "\n",
      "#################################################################################################\n",
      "#                                    yiyanghkust/finbert-esg                                    #\n",
      "################################################################################################# \n",
      "\n",
      "\n",
      "Prediction classes:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Environmental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Social</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Governance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Class\n",
       "0           None\n",
       "1  Environmental\n",
       "2         Social\n",
       "3     Governance"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum tokens allowed: 512\n",
      "\n",
      "Special Tokens:\n",
      "[PAD]: 0 \n",
      "[CLS]: 3 \n",
      "[SEP]: 4 \n",
      "[UNK]: 2\n",
      "\n",
      "\n",
      "-- Split sentences into chunks:\n",
      "\n",
      "Reloaded (evaluated in 0:05:26)\n",
      "\n",
      "Total sentences: 1866\n",
      "Total chunked sentences: 22852\n",
      "\n",
      "\n",
      "-- Query from API:\n",
      "\n",
      "Querying batch (50 rows) 458 / 458  last interaction: 19/02/2023 12:01:23  - total failed batch: 0 (0 rows)\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>OK</th>\n",
       "      <td>22852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Status\n",
       "OK   22852"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done in  5:50:00\n",
      "Data saved in .\\Checkpoints\\00_sentence_query_yiyanghkust_finbert-esg.pkl\n",
      "\n",
      "\n",
      "\n",
      "- Dataset saved to .\\Results\\02a_Sentiment_Raw_yiyanghkust_finbert-tone.csv\n",
      "- Dataset saved to .\\Results\\02a_Sentiment_Raw_yiyanghkust_finbert-esg-9-categories.csv\n",
      "- Dataset saved to .\\Results\\02a_Sentiment_Raw_yiyanghkust_finbert-esg.csv\n",
      "\n",
      "- Query log saved to .\\Checkpoints\\sentiment_raw.pkl\n"
     ]
    }
   ],
   "source": [
    "API_TOKEN = \"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\"\n",
    "API_URL = \"https://api-inference.huggingface.co/models/\"\n",
    "HEADERS = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "ROLLING_WINDOW_PERC=0.7\n",
    "QUERY_BATCH_SIZE=50   # batch size of sentences to be sent to API\n",
    "SPLIT_RELOAD=True\n",
    "QUERY_RELOAD=True\n",
    "\n",
    "MODEL_ID_LIST=['yiyanghkust/finbert-tone', 'yiyanghkust/finbert-esg-9-categories', 'yiyanghkust/finbert-esg']  # 'nbroad/ESG-BERT'\n",
    "\n",
    "\n",
    "query_log=sentence_classification(df_text, model_ID_list=MODEL_ID_LIST, rolling_window_perc=ROLLING_WINDOW_PERC,\n",
    "                                  query_batch_size=QUERY_BATCH_SIZE, split_reload=SPLIT_RELOAD,\n",
    "                                  query_reload=QUERY_RELOAD, cache_dir=CACHE_DIR, api_url=API_URL,\n",
    "                                  headers=HEADERS, checkpoint_folder=CHECKPOINT_FOLDER, sentiment_folder=SENTIMENT_FOLDER)\n",
    "\n",
    "print('\\n\\n')\n",
    "for k, v in query_log.items():\n",
    "    query_path_csv=os.path.join(RESULTS_FOLDER, '02a_Sentiment_Raw_'+k+'.csv')\n",
    "    v.to_csv(query_path_csv, index=False, sep=';')\n",
    "    print('- Dataset saved to', query_path_csv)\n",
    "    \n",
    "query_path=os.path.join(CHECKPOINT_FOLDER, 'sentiment_raw.pkl')\n",
    "joblib.dump(query_log, query_path, compress=('lzma', 3))\n",
    "print('\\n- Query log saved to', query_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9005f86f",
   "metadata": {},
   "source": [
    "### Average sentiment for each splitted sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3825500d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Dataset saved to .\\Results\\02b_Sentiment_Average_yiyanghkust_finbert-tone.csv\n",
      "- Dataset saved to .\\Results\\02b_Sentiment_Average_yiyanghkust_finbert-esg-9-categories.csv\n",
      "- Dataset saved to .\\Results\\02b_Sentiment_Average_yiyanghkust_finbert-esg.csv\n",
      "\n",
      "- Sentiment log saved to .\\Checkpoints\\sentiment_average.pkl\n"
     ]
    }
   ],
   "source": [
    "query_log=joblib.load(os.path.join(CHECKPOINT_FOLDER, 'sentiment_raw.pkl'))\n",
    "sentiment={}\n",
    "for mod, df in query_log.items():\n",
    "    df_avg=df.drop(columns=['Model', 'Chunk', 'Status', 'Error', 'ref_index', 'max', 'eval_time']).groupby('url').mean()\n",
    "    sentiment[mod]=df_avg\n",
    "    \n",
    "    path_csv=os.path.join(RESULTS_FOLDER, '02b_Sentiment_Average_'+mod+'.csv')\n",
    "    df_avg.to_csv(path_csv, index=False, sep=';')\n",
    "    print('- Dataset saved to', path_csv)\n",
    "    \n",
    "path=os.path.join(CHECKPOINT_FOLDER, 'sentiment_average.pkl')\n",
    "joblib.dump(sentiment, path, compress=('lzma', 3))\n",
    "print('\\n- Sentiment log saved to', path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25fcbbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130978a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8b03a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccdd2c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296e135f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ICOs)",
   "language": "python",
   "name": "ico"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
