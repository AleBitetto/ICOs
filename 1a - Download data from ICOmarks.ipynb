{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a4e15f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from utils import get_chromedriver, adjust_date, eval_duration, scrape_info_icomarks\n",
    "from utils import extract_scaping_icomarks, summary_stats, format_columns\n",
    "import requests\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "from soup2dict import convert\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "import datetime\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "import sys\n",
    "from thefuzz import fuzz\n",
    "from thefuzz import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62b7e9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMEDRIVER_PATH = r\"C:\\Users\\Alessandro Bitetto\\Downloads\\UniPV\\ICOs\\WebDriver\\chromedriver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "045f2684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set folders\n",
    "CHECKPOINT_FOLDER = '.\\\\Checkpoints'\n",
    "RESULTS_FOLDER = '.\\\\Results'\n",
    "ICOMARKS_FOLDER=os.path.join(CHECKPOINT_FOLDER, 'Icomarks')\n",
    "\n",
    "if not os.path.exists(CHECKPOINT_FOLDER):\n",
    "    os.makedirs(CHECKPOINT_FOLDER)\n",
    "if not os.path.exists(RESULTS_FOLDER):\n",
    "    os.makedirs(RESULTS_FOLDER)\n",
    "if not os.path.exists(ICOMARKS_FOLDER):\n",
    "    os.makedirs(ICOMARKS_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8e71a1",
   "metadata": {},
   "source": [
    "## Get ICOs url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd0676c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alessandro Bitetto\\AppData\\Local\\Temp\\ipykernel_19096\\2127388890.py:16: FutureWarning: In a future version of pandas all arguments of StringMethods.split except for the argument 'pat' will be keyword-only.\n",
      "  category[['Category', 'Count']] = category['Category'].str.split('(', 1, expand=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url_ref</th>\n",
       "      <th>Category</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>artificial-intelligence</td>\n",
       "      <td>AI</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>art</td>\n",
       "      <td>Art</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>banking</td>\n",
       "      <td>Banking</td>\n",
       "      <td>645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>big-data</td>\n",
       "      <td>Big Data</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business-services</td>\n",
       "      <td>Business</td>\n",
       "      <td>1340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>charity</td>\n",
       "      <td>Charity</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>communication</td>\n",
       "      <td>Communication</td>\n",
       "      <td>451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cryptocurrency</td>\n",
       "      <td>Cryptocurrency</td>\n",
       "      <td>2958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>defi</td>\n",
       "      <td>DeFi</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>education</td>\n",
       "      <td>Education</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>electronics</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>energy</td>\n",
       "      <td>Energy</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>exchange-launchpad</td>\n",
       "      <td>Exchange &amp; Launchpad</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>casino-gambling</td>\n",
       "      <td>Gambling</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>health</td>\n",
       "      <td>Health</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>infrastructure</td>\n",
       "      <td>Infrastructure</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>internet</td>\n",
       "      <td>Internet</td>\n",
       "      <td>575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>investment</td>\n",
       "      <td>Investment</td>\n",
       "      <td>1190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>legal</td>\n",
       "      <td>Legal</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>manufacturing</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>marketing-agency</td>\n",
       "      <td>Marketing Agency</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>media</td>\n",
       "      <td>Media</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>other</td>\n",
       "      <td>Other</td>\n",
       "      <td>428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>platform</td>\n",
       "      <td>Platform</td>\n",
       "      <td>3587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>real-estate</td>\n",
       "      <td>Real estate</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>retail</td>\n",
       "      <td>Retail</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>smart-contract</td>\n",
       "      <td>Smart Contract</td>\n",
       "      <td>795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>software</td>\n",
       "      <td>Software</td>\n",
       "      <td>789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>sports</td>\n",
       "      <td>Sports</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>tourism</td>\n",
       "      <td>Tourism</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>virtual-reality</td>\n",
       "      <td>Virtual Reality</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    url_ref               Category  Count\n",
       "0   artificial-intelligence                    AI     505\n",
       "1                       art                   Art      99\n",
       "2                   banking               Banking     645\n",
       "3                  big-data              Big Data     412\n",
       "4         business-services              Business    1340\n",
       "5                   charity               Charity     156\n",
       "6             communication         Communication     451\n",
       "7            cryptocurrency        Cryptocurrency    2958\n",
       "8                      defi                  DeFi     489\n",
       "9                 education             Education     217\n",
       "10              electronics           Electronics     102\n",
       "11                   energy                Energy     199\n",
       "12            entertainment         Entertainment     660\n",
       "13       exchange-launchpad  Exchange & Launchpad     137\n",
       "14          casino-gambling              Gambling     200\n",
       "15                   health                Health     314\n",
       "16           infrastructure        Infrastructure     560\n",
       "17                 internet              Internet     575\n",
       "18               investment            Investment    1190\n",
       "19                    legal                 Legal      95\n",
       "20            manufacturing         Manufacturing     160\n",
       "21         marketing-agency      Marketing Agency       8\n",
       "22                    media                 Media     400\n",
       "23                    other                 Other     428\n",
       "24                 platform              Platform    3587\n",
       "25              real-estate           Real estate     257\n",
       "26                   retail                Retail     316\n",
       "27           smart-contract        Smart Contract     795\n",
       "28                 software              Software     789\n",
       "29                   sports                Sports     166\n",
       "30                  tourism               Tourism     198\n",
       "31                  unknown               Unknown      15\n",
       "32          virtual-reality       Virtual Reality     154"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAIN_PAGE = \"https://icomarks.com/\"                # to be added to single ICO url\n",
    "CATEGORY_PAGE = \"https://icomarks.com/icos/\"       # used to query the category to be downloaded\n",
    "\n",
    "# get html\n",
    "page = requests.get(CATEGORY_PAGE)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# extract list of categories\n",
    "tag = soup.find_all('div', class_=\"icoTop__selects\", recursive=True)\n",
    "conv_dict = convert(tag)\n",
    "while conv_dict['div'][0]['@class'][0] != 'icoTop__selects':\n",
    "    conv_dict = conv_dict['div'][0]\n",
    "category_list = conv_dict['div'][0]['form'][0]['select'][0]['option']\n",
    "category = pd.DataFrame([(v['@value'], v['#text']) for v in category_list if '@value' in v.keys()],\n",
    "                        columns =['url_ref', 'Category'])\n",
    "category[['Category', 'Count']] = category['Category'].str.split('(', 1, expand=True)\n",
    "category['Count'] = category['Count'].apply(lambda x: int(x.replace(')', '')))\n",
    "display(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9204d356",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Downloading: AI   (505 expected) 1 / 33\n",
      "   - Scrolling down...OK\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...OK\n",
      "\n",
      "- Downloading: Art   (99 expected) 2 / 33\n",
      "   - Scrolling down...OK\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...OK\n",
      "\n",
      "- Downloading: Banking   (645 expected) 3 / 33\n",
      "   - Scrolling down...OK\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...OK\n",
      "\n",
      "- Downloading: Big Data   (412 expected) 4 / 33\n",
      "   - Scrolling down...OK\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...OK\n",
      "\n",
      "- Downloading: Business   (1341 expected) 5 / 33\n",
      "   - Scrolling down...OK\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...OK\n",
      "\n",
      "- Downloading: Charity   (156 expected) 6 / 33\n",
      "   - Scrolling down...OK\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...OK\n",
      "\n",
      "- Downloading: Communication   (451 expected) 7 / 33\n",
      "   - Scrolling down...OK\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...OK\n",
      "\n",
      "- Downloading: Cryptocurrency   (2959 expected) 8 / 33\n",
      "   - Scrolling down...OK\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...OK\n",
      "\n",
      "- Downloading: DeFi   (489 expected) 9 / 33\n",
      "   - Scrolling down...OK\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...   ####### warning, expected number of elements (489) mismatch. Found 490\n",
      "\n",
      "- Downloading: Education   (217 expected) 10 / 33\n",
      "   - Scrolling down...OK\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...OK\n",
      "\n",
      "- Downloading: Electronics   (102 expected) 11 / 33\n",
      "   - Scrolling down...OK\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...OK\n",
      "\n",
      "- Downloading: Energy   (199 expected) 12 / 33\n",
      "   - Scrolling down...OK\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...OK\n",
      "\n",
      "- Downloading: Entertainment   (661 expected) 13 / 33\n",
      "   - Scrolling down...OK\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...OK\n",
      "\n",
      "- Downloading: Exchange & Launchpad   (137 expected) 14 / 33\n",
      "   - Scrolling down...OK\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...OK\n",
      "\n",
      "- Downloading: Gambling   (200 expected) 15 / 33\n",
      "   - Scrolling down...OK\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...OK\n",
      "\n",
      "- Downloading: Health   (314 expected) 16 / 33\n",
      "   - Scrolling down...OK\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...OK\n",
      "\n",
      "- Downloading: Infrastructure   (560 expected) 17 / 33\n",
      "   - Scrolling down...OK\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...OK\n",
      "\n",
      "- Downloading: Internet   (575 expected) 18 / 33\n",
      "   - Scrolling down...OK\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...OK\n",
      "\n",
      "- Downloading: Investment   (1190 expected) 19 / 33\n",
      "   - Scrolling down...OK\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...OK\n",
      "\n",
      "- Downloading: Legal   (95 expected) 20 / 33\n",
      "   - Scrolling down...OK\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...OK\n",
      "\n",
      "- Downloading: Manufacturing   (160 expected) 21 / 33\n",
      "   - Scrolling down...OK\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...OK\n",
      "\n",
      "- Downloading: Marketing Agency   (8 expected) 22 / 33\n",
      "   - Scrolling down...SKIPPED\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...OK\n",
      "\n",
      "- Downloading: Media   (400 expected) 23 / 33\n",
      "   - Scrolling down...OK\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...OK\n",
      "\n",
      "- Downloading: Other   (428 expected) 24 / 33\n",
      "   - Scrolling down...OK\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...OK\n",
      "\n",
      "- Downloading: Platform   (3587 expected) 25 / 33\n",
      "   - Scrolling down...OK\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...   ####### warning, expected number of elements (3587) mismatch. Found 3588\n",
      "\n",
      "- Downloading: Real estate   (257 expected) 26 / 33\n",
      "   - Scrolling down...OK\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...OK\n",
      "\n",
      "- Downloading: Retail   (316 expected) 27 / 33\n",
      "   - Scrolling down...OK\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...OK\n",
      "\n",
      "- Downloading: Smart Contract   (796 expected) 28 / 33\n",
      "   - Scrolling down...OK\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...   ####### warning, expected number of elements (796) mismatch. Found 797\n",
      "\n",
      "- Downloading: Software   (789 expected) 29 / 33\n",
      "   - Scrolling down...OK\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...OK\n",
      "\n",
      "- Downloading: Sports   (166 expected) 30 / 33\n",
      "   - Scrolling down...OK\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...OK\n",
      "\n",
      "- Downloading: Tourism   (198 expected) 31 / 33\n",
      "   - Scrolling down...OK\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...OK\n",
      "\n",
      "- Downloading: Unknown   (15 expected) 32 / 33\n",
      "   - Scrolling down...SKIPPED\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...OK\n",
      "\n",
      "- Downloading: Virtual Reality   (154 expected) 33 / 33\n",
      "   - Scrolling down...OK\n",
      "   - Downloading html...OK\n",
      "   - Parsing info...OK\n",
      "\n",
      "\n",
      "Total elapsed time: 1:08:45\n",
      "\n",
      "Data saved in  ./Results/01a_ICOmarks_ico_list_raw.csv\n"
     ]
    }
   ],
   "source": [
    "# apply category in search query and get ICO list\n",
    "\n",
    "cat_list = pd.DataFrame(columns=['Category', 'url', 'NViews', 'VerifiedEmailDummy', 'IsSTODummy', 'IsIEODummy',\n",
    "                                 'Status', 'StartDate', 'EndDate'])\n",
    "start = timer()\n",
    "download_date=datetime.datetime.now().strftime(\"%d/%m/%Y\")\n",
    "for index, row in category.iterrows():\n",
    "    \n",
    "    url_categ = row['url_ref']\n",
    "    expected_count = row['Count']\n",
    "    categ = row['Category']\n",
    "    \n",
    "    print('\\n- Downloading: ' + categ + '  ('+ str(expected_count) + ' expected) ' + str(index + 1) + ' / ' + str(len(category)))\n",
    "    \n",
    "    # scroll down till \"Show more\" button disappear\n",
    "    show_more_path = '/html/body/section/div[2]/div[2]/div[2]/a'\n",
    "\n",
    "    print('   - Scrolling down...', end ='')\n",
    "\n",
    "    driver = get_chromedriver(chromedriver_path = CHROMEDRIVER_PATH)\n",
    "    driver.get(urljoin(CATEGORY_PAGE, url_categ))\n",
    "\n",
    "    try:\n",
    "        while driver.find_element(\"xpath\", show_more_path).is_displayed():\n",
    "\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", driver.find_element(\"xpath\", show_more_path))\n",
    "            driver.find_element(\"xpath\", show_more_path).click()\n",
    "            time.sleep(3)\n",
    "        print('OK')\n",
    "    except:\n",
    "        print('SKIPPED')\n",
    "\n",
    "    # get html\n",
    "    print('   - Downloading html...', end='')\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    print('OK')\n",
    "\n",
    "    # extract information from web list\n",
    "    print('   - Parsing info...', end='')\n",
    "    tag = soup.find_all('div', class_=\"icoListContent\", recursive=True)\n",
    "    tag_list = []\n",
    "    for t in tag[0]:\n",
    "        if 'div class=\"newItems\"' not in str(t):\n",
    "            tag_list.append(t)\n",
    "    # if show more, html structure changes\n",
    "    nested_tags=soup.find_all('div', class_=\"newItems\", recursive=True)\n",
    "    if len(nested_tags) > 0:\n",
    "        for x in soup.find_all('div', class_=\"newItems\", recursive=True):\n",
    "            tag_list.extend([y for y in x])\n",
    "\n",
    "    temp_list = pd.DataFrame(columns=cat_list.columns)\n",
    "    for t in tag_list:\n",
    "\n",
    "        if 'START' in str(t):\n",
    "\n",
    "            conv_dict = convert(t)['div']\n",
    "            for x in conv_dict:\n",
    "\n",
    "                if x['@class'][0] == 'icoListItem__info':\n",
    "                    sup = x['a'][0]['sup']\n",
    "                    n_views = [y['#text'] for y in sup if y['@class'][0] == \"sup_views\"][0]\n",
    "                    is_sto = int(any([True if y['@class'][0] == \"sup_is_sto\" else False for y in sup]))\n",
    "                    is_ieo = int(any([True if y['@class'][0] == \"sup_is_ieo\" else False for y in sup]))\n",
    "                    ver_email = int(any([True if y['@class'][0] == \"sup_email_confirmed\" else False for y in sup]))\n",
    "                    url = x['a'][0]['@href']\n",
    "                if x['@class'][0] == 'icoListItem__raised':\n",
    "                    status = x['#text']#[v['#text'] for k, v in x['span'][0].items() if ]\n",
    "                if x['@class'][0] == 'icoListItem__start':\n",
    "                    start_date = x['navigablestring'][0]\n",
    "                if x['@class'][0] == 'icoListItem__end':\n",
    "                    end_date = x['navigablestring'][0]\n",
    "\n",
    "            temp_list = temp_list.append(pd.DataFrame({\n",
    "                'Category':categ,\n",
    "                'url': urljoin(MAIN_PAGE, url),\n",
    "                'NViews': int(n_views.replace(' Views', '').replace(',', '')),\n",
    "                'VerifiedEmailDummy': ver_email,\n",
    "                'IsSTODummy': is_sto,\n",
    "                'IsIEODummy': is_ieo,\n",
    "                'Status': status.replace('STATUS ', ''),\n",
    "                'StartDate': start_date,\n",
    "                'EndDate': end_date\n",
    "            }, index = [0]))\n",
    "\n",
    "    temp_list['ListDownloadedOn']=download_date\n",
    "    cat_list = cat_list.append(temp_list)\n",
    "    \n",
    "    # save results\n",
    "    cat_list.to_csv(os.path.join(RESULTS_FOLDER,'01a_ICOmarks_ico_list.csv'), index=False, sep=';')\n",
    "\n",
    "    if temp_list.shape[0] != expected_count:\n",
    "        print('   ####### warning, expected number of elements (' + str(expected_count) + ') mismatch. Found ' + str(temp_list.shape[0]))\n",
    "    else:\n",
    "        print('OK')\n",
    "\n",
    "    driver.close()\n",
    "    \n",
    "print('\\n\\nTotal elapsed time:', str(datetime.timedelta(seconds=round(timer()-start))))\n",
    "print('\\nData saved in ', os.path.join(RESULTS_FOLDER,'01a_ICOmarks_ico_list_raw.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1a32b8",
   "metadata": {},
   "source": [
    "### Check downloaded list and remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9d2c76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Url with multiple entries found. Keeping single information only\n",
      "    - https://icomarks.com/ico/ins: Multiple status found: ['Trading' 'Ended']. Keeping 'Ended'\n",
      "    - https://icomarks.com/ico/unifox: Multiple status found: ['Pre-Sale Ended' 'Ended']. Keeping 'Ended'\n",
      "\n",
      "-- 7 rows with error in \"LogDurationDays\":\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Upcoming</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Status\n",
       "Upcoming       7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log saved in  .\\Results\\01b_ICOmarks_ico_list_adjusted_DurationError.csv\n",
      "\n",
      "-- Total ICOs found: 8279\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ended</th>\n",
       "      <td>5034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Upcoming</th>\n",
       "      <td>1890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trading</th>\n",
       "      <td>726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pre-Sale Ended</th>\n",
       "      <td>399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Active</th>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pre-Sale</th>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Status\n",
       "Ended             5034\n",
       "Upcoming          1890\n",
       "Trading            726\n",
       "Pre-Sale Ended     399\n",
       "Active             164\n",
       "Pre-Sale            66"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data saved in  .\\Results\\01b_ICOmarks_ico_list_adjusted.csv\n"
     ]
    }
   ],
   "source": [
    "cat_list=pd.read_csv(os.path.join(RESULTS_FOLDER,'01a_ICOmarks_ico_list_raw.csv'), sep=';')\n",
    "\n",
    "cat_list.drop_duplicates(inplace=True)\n",
    "\n",
    "# adjust dates\n",
    "cat_list['StartDate']=cat_list['StartDate'].map(adjust_date)\n",
    "cat_list['EndDate']=cat_list['EndDate'].map(adjust_date)\n",
    "\n",
    "# find url with multiple entries (due to IEO/STO) and keep all categories and minimum start date and max end date\n",
    "multiple_url=cat_list[['url', 'NViews']].drop_duplicates()['url'].value_counts().to_frame().reset_index().query('url > 1')['index']\n",
    "if len(multiple_url) > 0:\n",
    "    \n",
    "    print('\\n-- Url with multiple entries found. Keeping single information only')\n",
    "    new_df=pd.DataFrame(columns=cat_list.columns)\n",
    "    for t_url in multiple_url:\n",
    "        t_df=cat_list[cat_list['url']==t_url].copy()\n",
    "\n",
    "        status=t_df['Status'].value_counts().index[0]\n",
    "        if t_df['Status'].nunique() > 1:\n",
    "            u_val=t_df['Status'].unique()\n",
    "            if 'Ended' in u_val:\n",
    "                status='Ended'\n",
    "            if 'Active' in u_val:\n",
    "                status='Active'\n",
    "            print(f\"    - {t_url}: Multiple status found: {u_val}. Keeping '{status}'\")\n",
    "\n",
    "        try:\n",
    "            start_date=pd.to_datetime(t_df['StartDate'].loc[lambda x : x != 'TBA'], infer_datetime_format=True).min().strftime('%d %b %Y')\n",
    "        except:\n",
    "            start_date=t_df['StartDate'].unique()[0]\n",
    "        try:\n",
    "            end_date=pd.to_datetime(t_df['EndDate'].loc[lambda x : x != 'TBA'], infer_datetime_format=True).max().strftime('%d %b %Y')\n",
    "        except:\n",
    "            end_date=t_df['EndDate'].unique()[0]\n",
    "\n",
    "        add_df=pd.DataFrame({'Category': t_df['Category'].unique(),\n",
    "                            'url': t_url,\n",
    "                            'NViews': t_df['NViews'].max(),\n",
    "                            'VerifiedEmailDummy': t_df['VerifiedEmailDummy'].max(),\n",
    "                            'IsSTODummy': t_df['IsSTODummy'].max(),\n",
    "                            'IsIEODummy': t_df['IsIEODummy'].max(),\n",
    "                            'Status': status,\n",
    "                            'StartDate': start_date,\n",
    "                            'EndDate': end_date,\n",
    "                            'ListDownloadedOn': t_df['ListDownloadedOn'].values[0]})\n",
    "\n",
    "        new_df=pd.concat([new_df, add_df])\n",
    "        \n",
    "    cat_list=cat_list[~cat_list['url'].isin(multiple_url)]\n",
    "    cat_list=pd.concat([cat_list, new_df]) \n",
    "\n",
    "# get dummy for category\n",
    "cat_list['Category']=cat_list['Category'].str.replace(' ', '')\n",
    "cat_dummy=pd.concat([cat_list['url'], pd.get_dummies(cat_list['Category'], drop_first=False, prefix='Category', prefix_sep='')], axis=1)\n",
    "cat_dummy=cat_dummy.groupby('url').sum()\n",
    "cat_dummy.columns=cat_dummy.columns+'Dummy'\n",
    "if cat_dummy.max().max() != 1:\n",
    "    print('\\n ### \"Category\" dummy variable has value greater than 1')\n",
    "cat_dummy.reset_index(inplace=True)\n",
    "\n",
    "# evaluate duration\n",
    "cat_list.loc[cat_list['url'] == 'https://icomarks.com/ico/kaizen-coin', 'StartDate']='18 Aug 2017'\n",
    "cat_list.loc[cat_list['url'] == 'https://icomarks.com/ico/curveblock', 'EndDate']='31-mar-19'\n",
    "cat_list.loc[cat_list['url'] == 'https://icomarks.com/ico/0chain', 'EndDate']='19-feb-18'\n",
    "cat_list.loc[cat_list['url'] == 'https://icomarks.com/ico/hunibit', 'StartDate']='21 apr 2019'\n",
    "cat_list.loc[cat_list['url'] == 'https://icomarks.com/ico/hunibit', 'EndDate']='03 may 2019'\n",
    "cat_list.loc[cat_list['url'] == 'https://icomarks.com/ico/clearaid', 'EndDate']='01 may 2019'\n",
    "cat_list.loc[cat_list['url'] == 'https://icomarks.com/ico/ultrashares', 'StartDate']='23 apr 2018'\n",
    "cat_list.loc[cat_list['url'] == 'https://icomarks.com/ico/ultrashares', 'EndDate']='30 jun 2018'\n",
    "cat_list.loc[cat_list['url'] == 'https://icomarks.com/ico/dentix', 'StartDate']='01-mar-18'\n",
    "cat_list.loc[cat_list['url'] == 'https://icomarks.com/ico/eos', 'EndDate']='04 jun 2018'\n",
    "cat_list.loc[cat_list['url'] == 'https://icomarks.com/ico/mundus', 'StartDate']='31 aug 2017'\n",
    "cat_list.loc[cat_list['url'] == 'https://icomarks.com/ico/mundus', 'EndDate']='30 oct 2017'\n",
    "cat_list.loc[cat_list['url'] == 'https://icomarks.com/ico/vr-park', 'StartDate']='24 aug 2019'\n",
    "cat_list.loc[cat_list['url'] == 'https://icomarks.com/ico/vr-park', 'EndDate']='17 apr 2020'\n",
    "cat_list.loc[cat_list['url'] == 'https://icomarks.com/ico/goldminecoin', 'EndDate']='15 mar 2018'\n",
    "cat_list.loc[cat_list['url'] == 'https://icomarks.com/ico/horsechain', 'EndDate']='14 Jul 2019'\n",
    "cat_list['LogDurationDays']=cat_list.apply(eval_duration, axis=1)\n",
    "move_col = cat_list.pop('LogDurationDays')\n",
    "cat_list.insert(cat_list.columns.get_loc(\"EndDate\")+1, 'LogDurationDays', move_col)\n",
    "check_error=cat_list[cat_list['LogDurationDays'] < 0][['url', 'Status', 'StartDate', 'EndDate']].drop_duplicates()\n",
    "if len(check_error):\n",
    "    print(f'\\n-- {len(check_error)} rows with error in \"LogDurationDays\":')\n",
    "    display(check_error['Status'].value_counts().to_frame())\n",
    "    check_error.to_csv(os.path.join(RESULTS_FOLDER, '01b_ICOmarks_ico_list_adjusted_DurationError.csv'), index=False, sep=';')\n",
    "    print('Log saved in ', os.path.join(RESULTS_FOLDER, '01b_ICOmarks_ico_list_adjusted_DurationError.csv'))\n",
    "    \n",
    "\n",
    "# create final dataset\n",
    "cat_list=cat_list.drop(columns='Category').drop_duplicates()\n",
    "cat_list=cat_list.merge(cat_dummy, on='url', how='left')\n",
    "move_col = cat_list.pop('ListDownloadedOn')\n",
    "cat_list.insert(cat_list.columns.get_loc(\"url\")+1, 'ListDownloadedOn', move_col)\n",
    "\n",
    "if cat_list['url'].nunique() != cat_list.shape[0]:\n",
    "    print('\\n ##### Unique urls do not match number of rows')\n",
    "\n",
    "print('\\n-- Total ICOs found:', cat_list['url'].nunique())\n",
    "display(cat_list['Status'].value_counts().to_frame())\n",
    "    \n",
    "# save csv\n",
    "cat_list.to_csv(os.path.join(RESULTS_FOLDER, '01b_ICOmarks_ico_list_adjusted.csv'), index=False, sep=';')\n",
    "print('\\nData saved in ', os.path.join(RESULTS_FOLDER, '01b_ICOmarks_ico_list_adjusted.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13017da",
   "metadata": {},
   "source": [
    "## Scrape information from url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f6c92ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_ROOT='https://icomarks.com/ico/'    # will be removed from url to save pickle in ICOMARKS_FOLDER\n",
    "RELOAD_PKL=True\n",
    "SKIP_MISSING=False     # if True skip attempt to scrape missing pickles\n",
    "\n",
    "if not os.path.exists(ICOMARKS_FOLDER):\n",
    "    os.makedirs(ICOMARKS_FOLDER)\n",
    "\n",
    "cat_list=pd.read_csv(os.path.join(RESULTS_FOLDER, '01b_ICOmarks_ico_list_adjusted.csv'), sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8ab4387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Scraping: 8279 / 8279   last interaction: 08/02/2023 23:09:02\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ScrapeStatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>OK</th>\n",
       "      <td>8279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ScrapeStatus\n",
       "OK          8279"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Social Media time series status:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SocialSeriesStatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DOWNLOADED</th>\n",
       "      <td>6398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOCIAL_TAB_MISSING</th>\n",
       "      <td>1654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOWNLOAD_NOT_AVAILABLE</th>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        SocialSeriesStatus\n",
       "DOWNLOADED                            6398\n",
       "SOCIAL_TAB_MISSING                    1654\n",
       "DOWNLOAD_NOT_AVAILABLE                 227"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Market Price time series status:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MarketPriceSeriesStatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DOWNLOAD_NOT_AVAILABLE</th>\n",
       "      <td>7555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOWNLOADED</th>\n",
       "      <td>724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        MarketPriceSeriesStatus\n",
       "DOWNLOAD_NOT_AVAILABLE                     7555\n",
       "DOWNLOADED                                  724"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Total elapsed time: 1 day, 9:00:33\n",
      "\n",
      "Data saved in .\\Checkpoints\\scrape_df_raw.pkl\n"
     ]
    }
   ],
   "source": [
    "scrape_df=pd.DataFrame()\n",
    "for index, row in cat_list.iterrows():\n",
    "    \n",
    "    print(f'- Scraping: {str(index + 1)} / {len(cat_list)}   last interaction: {datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}', end='\\r')\n",
    "    \n",
    "    url=row['url']\n",
    "    save_path=os.path.join(ICOMARKS_FOLDER, url.replace(URL_ROOT, '')+'.json').replace('|', '')\n",
    "    \n",
    "    if not RELOAD_PKL or not os.path.exists(save_path):\n",
    "    \n",
    "        if SKIP_MISSING and not os.path.exists(save_path):\n",
    "            add_row=pd.DataFrame({'url': url, 'ScrapeStatus': 'ERROR'}, index=[0])\n",
    "            scrape_df=pd.concat([scrape_df, add_row])\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            start = timer()\n",
    "            add_row=scrape_info_icomarks(url=url, chromedriver_path=CHROMEDRIVER_PATH, skip_social=False, skip_price=False)\n",
    "            add_row.insert(1, 'ScrapeStatus', 'OK')\n",
    "            add_row['PklPath']=save_path\n",
    "            add_row['TotTimeSec']=datetime.timedelta(seconds=round(timer()-start)).total_seconds()\n",
    "            add_row.to_json(save_path, orient='table')\n",
    "        except:\n",
    "            add_row=pd.DataFrame({'url': url, 'ScrapeStatus': 'ERROR'}, index=[0])\n",
    "    \n",
    "    else:\n",
    "        add_row=pd.read_json(save_path, orient='table')\n",
    "        # re-format nested dataframe from json schema\n",
    "        add_row['InfoBlock']=[pd.DataFrame(add_row['InfoBlock'][0])]\n",
    "        if 'TeamBlock' in add_row.columns:\n",
    "            add_row['TeamBlock']=[pd.DataFrame(add_row['TeamBlock'][0])]\n",
    "        if 'SocialBlock' in add_row.columns:\n",
    "            social_df=pd.DataFrame(add_row['SocialBlock'][0][0]['stats'])\n",
    "            series_dict={}\n",
    "            for k in add_row['SocialBlock'][0][0]['timeseries'].keys():\n",
    "                series_dict[k]=pd.DataFrame(add_row['SocialBlock'][0][0]['timeseries'][k])\n",
    "            add_row['SocialBlock']=[[{'stats': social_df, 'timeseries': series_dict}]]\n",
    "        if 'MarketPriceSeries' in add_row.columns:\n",
    "            add_row['MarketPriceSeries']=[pd.DataFrame(add_row['MarketPriceSeries'][0])]\n",
    "        \n",
    "    scrape_df=pd.concat([scrape_df, add_row])\n",
    "\n",
    "scrape_df.reset_index(drop=True, inplace=True)\n",
    "display(scrape_df['ScrapeStatus'].value_counts().to_frame())\n",
    "   \n",
    "print('\\n\\nTotal elapsed time:', str(datetime.timedelta(seconds=round(scrape_df['TotTimeSec'].sum()))))\n",
    "\n",
    "# save\n",
    "pkl_path=os.path.join(CHECKPOINT_FOLDER, 'scrape_df_raw.pkl')\n",
    "joblib.dump(scrape_df, pkl_path, compress=('lzma', 3))\n",
    "print(f'\\nData saved in {pkl_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ac6c3f",
   "metadata": {},
   "source": [
    "## Format scraped information and save final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43890678",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----######   Extracting information from raw scraped data   ######----\n",
      "\n",
      "   - Processing 8279 / 8279\n",
      "   - Data saved in .\\Checkpoints\\scrape_df_extracted.pkl\n",
      "\n",
      "\n",
      "\n",
      "----######   Formatting columns   ######----\n",
      "\n",
      "** Formatting \"FundRaised\"\n",
      "\n",
      "** Formatting \"Country\"\n",
      "- Mapped countries with low accuracy:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Country_adj</th>\n",
       "      <th>country</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Grand Cayman</td>\n",
       "      <td>Grand Cayman</td>\n",
       "      <td>Cayman Islands</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>Singapura</td>\n",
       "      <td>Singapura</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>Singarope</td>\n",
       "      <td>Singarope</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Malte</td>\n",
       "      <td>Malte</td>\n",
       "      <td>Malta</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>Nederland</td>\n",
       "      <td>Nederland</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>Melta</td>\n",
       "      <td>Melta</td>\n",
       "      <td>Malta</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Brasil</td>\n",
       "      <td>Brasil</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Сanada</td>\n",
       "      <td>Сanada</td>\n",
       "      <td>Canada</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>Tunis</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>Tunisia</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>St Vincent</td>\n",
       "      <td>St Vincent</td>\n",
       "      <td>Saint Vincent and the Grenadines</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>Perú</td>\n",
       "      <td>Perú</td>\n",
       "      <td>Peru</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Vietnum</td>\n",
       "      <td>Vietnum</td>\n",
       "      <td>Vietnam</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Cambidia</td>\n",
       "      <td>Cambidia</td>\n",
       "      <td>Cambodia</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Cameroun</td>\n",
       "      <td>Cameroun</td>\n",
       "      <td>Cameroon</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Singapure</td>\n",
       "      <td>Singapure</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Country   Country_adj                           country  accuracy\n",
       "98   Grand Cayman  Grand Cayman                    Cayman Islands        73\n",
       "231     Singapura     Singapura                         Singapore        78\n",
       "256     Singarope     Singarope                         Singapore        78\n",
       "160         Malte         Malte                             Malta        80\n",
       "167     Nederland     Nederland                       Netherlands        80\n",
       "222         Melta         Melta                             Malta        80\n",
       "85         Brasil        Brasil                            Brazil        83\n",
       "159        Сanada        Сanada                            Canada        83\n",
       "259         Tunis         Tunis                           Tunisia        83\n",
       "174    St Vincent    St Vincent  Saint Vincent and the Grenadines        86\n",
       "213          Perú          Perú                              Peru        86\n",
       "249       Vietnum       Vietnum                           Vietnam        86\n",
       "177      Cambidia      Cambidia                          Cambodia        88\n",
       "180      Cameroun      Cameroun                          Cameroon        88\n",
       "114     Singapure     Singapure                         Singapore        89"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "** Formatting \"SocialMedia\"\n",
      "- Counts for SocialMedia dummy:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Twitter</td>\n",
       "      <td>7677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Telegram</td>\n",
       "      <td>6915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>6030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Medium</td>\n",
       "      <td>4619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Youtube</td>\n",
       "      <td>4001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bitcointalk</td>\n",
       "      <td>3836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Reddit</td>\n",
       "      <td>3588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Github</td>\n",
       "      <td>3214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Linkedin</td>\n",
       "      <td>1169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Instagram</td>\n",
       "      <td>1041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Discord</td>\n",
       "      <td>922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Slack</td>\n",
       "      <td>867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>VK</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            val  count\n",
       "10      Twitter   7677\n",
       "9      Telegram   6915\n",
       "2      Facebook   6030\n",
       "6        Medium   4619\n",
       "12      Youtube   4001\n",
       "0   Bitcointalk   3836\n",
       "7        Reddit   3588\n",
       "3        Github   3214\n",
       "5      Linkedin   1169\n",
       "4     Instagram   1041\n",
       "1       Discord    922\n",
       "8         Slack    867\n",
       "11           VK    304"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "** Formatting \"ICOPrice\", \"IEOPrice\", \"STOPrice\"\n",
      "- Errors when parsing \"ICOPrice\", \"IEOPrice\", \"STOPrice\":\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                                    6398\n",
       "currency range error missing label-missing currency unit numeric      13\n",
       "missing currency unit numeric                                         10\n",
       "multiple token or currency index                                       3\n",
       "currency error float-missing currency unit numeric                     2\n",
       "multiple token or currency index -missing currency unit numeric        1\n",
       "Name: error, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Error log saved in .\\Results\\01c_ICOmarks_formatted_price_error_log.csv\n",
      "- 17 rows removed because currency FX rate not available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QTUM       1\n",
       "GOFGOLD    1\n",
       "WAN        1\n",
       "CENTS      1\n",
       "USA        1\n",
       "CAD        1\n",
       "BSCX       1\n",
       "FTM        1\n",
       "ET         1\n",
       "TTC        1\n",
       "TH         1\n",
       "RMB        1\n",
       "BTCM       1\n",
       "XEM        1\n",
       "JPY        1\n",
       "GA         1\n",
       "ADA        1\n",
       "Name: currency_lab, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Taking closest available FX rate for 7 rows. Currency: ['VET' 'KRW' 'USDT']\n",
      "- Price available for 6381 entries\n",
      "- Price log saved in .\\Results\\01c_ICOmarks_formatted_price_log.csv\n",
      "\n",
      "** Formatting \"PreSalePrice\"\n",
      "- Errors when parsing \"PreSalePrice\":\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                                    1697\n",
       "currency range error missing label-missing currency unit numeric       8\n",
       "missing currency unit numeric                                          4\n",
       "multiple token or currency index                                       1\n",
       "Name: error, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Error log saved in .\\Results\\01c_ICOmarks_formatted_PreSaleprice_error_log.csv\n",
      "- 3 rows removed because currency FX rate not available\n",
      "- Taking closest available FX rate for 1 rows. Currency: ['USDT']\n",
      "- Price available for 1694 entries\n",
      "- Price log saved in .\\Results\\01c_ICOmarks_formatted_PreSaleprice_log.csv\n",
      "\n",
      "** Formatting \"FundHardCap\" and \"FundSoftCap\"\n",
      "- Errors when parsing \"FundHardCap\" and \"FundSoftCap\":\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                                      7769\n",
       "missing currency unit numeric                                           26\n",
       "currency range error missing label-missing currency unit numeric        23\n",
       "currency range error multiple labels-missing currency unit numeric       1\n",
       "multiple token or currency index                                         1\n",
       "Name: error, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Error log saved in .\\Results\\01c_ICOmarks_formatted_HardSoftCap_error_log.csv\n",
      "- 25 rows in \"only_token_df\" (with Hard/Soft Cap in tokens) skipped because of missing \"PriceUSD\"\n",
      "- 475 rows remaing in \"only_token_df\" (with Hard/Soft Cap in tokens)\n",
      "- 23 rows in \"only_currency_df\" (with Hard/Soft Cap in currency) skipped because FX rate not available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MILLION          4\n",
       "WEEDO            2\n",
       "PLN              2\n",
       "MATRIX           2\n",
       "DCO              1\n",
       "BCC              1\n",
       "MATC             1\n",
       "GC               1\n",
       "USO              1\n",
       "OFTOTALSUPPLY    1\n",
       "DTC              1\n",
       "US               1\n",
       "M                1\n",
       "TOKENS           1\n",
       "QTUM             1\n",
       "IDAP             1\n",
       "PXS              1\n",
       "Name: currency_lab, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Taking closest available FX rate for 1 rows. Currency: ['KRW']\n",
      "- 7242 rows remaing in \"only_currency_df\" (with Hard/Soft Cap in currency)\n",
      "- All 4 rows in \"both_df\" (with Hard/Soft Cap in currency AND token) skipped because of mismatch\n",
      "\n",
      "- Hard/Soft Cap available for 7717 entries\n",
      "- Price log saved in .\\Results\\01c_ICOmarks_formatted_HardSoftCap_log.csv\n",
      "\n",
      "\n",
      "\n",
      "----######   Merging with category dataset   ######----\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ended</th>\n",
       "      <td>5034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Upcoming</th>\n",
       "      <td>1890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trading</th>\n",
       "      <td>726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pre-Sale Ended</th>\n",
       "      <td>399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Active</th>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pre-Sale</th>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Status\n",
       "Ended             5034\n",
       "Upcoming          1890\n",
       "Trading            726\n",
       "Pre-Sale Ended     399\n",
       "Active             164\n",
       "Pre-Sale            66"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Total rows: 8279\n",
      "\n",
      "   - Data saved in .\\Results\\01d_ICOmarks_ico_list_scraped_formatted.csv\n",
      "   - Pickle saved in .\\Checkpoints\\formatted_df.pkl\n",
      "   - Stats saved in .\\Results\\01d_ICOmarks_ico_list_scraped_formatted_Stats.csv\n",
      "\n",
      "\n",
      "Total elapsed time: 0:12:10\n"
     ]
    }
   ],
   "source": [
    "pkl_path=os.path.join(CHECKPOINT_FOLDER, 'scrape_df_raw.pkl')\n",
    "scrape_df=joblib.load(pkl_path)\n",
    "\n",
    "start=timer()\n",
    "\n",
    "#### extract from raw data\n",
    "print('----######   Extracting information from raw scraped data   ######----\\n')\n",
    "format_df=extract_scaping_icomarks(scrape_df).reset_index(drop=True)\n",
    "\n",
    "pkl_path=os.path.join(CHECKPOINT_FOLDER, 'scrape_df_extracted.pkl')\n",
    "joblib.dump(format_df, pkl_path, compress=('lzma', 3))\n",
    "print(f'\\n   - Data saved in {pkl_path}')\n",
    "\n",
    "\n",
    "#### format nested column and merge with cat_list dataset\n",
    "cat_list=pd.read_csv(os.path.join(RESULTS_FOLDER, '01b_ICOmarks_ico_list_adjusted.csv'), sep=';')\n",
    "print('\\n\\n\\n----######   Formatting columns   ######----')\n",
    "if len(cat_list) != len(format_df):\n",
    "    raise ValueError('\\n\\n ########### Error: \"cat_list\" and \"format_df\" must have same rows')\n",
    "format_df_rows=format_df.shape[0]\n",
    "format_df=format_columns(format_df, cat_list=cat_list, format_df_rows=format_df_rows, results_folder=RESULTS_FOLDER)\n",
    "final_df['WebsiteUrl']=final_df['WebsiteUrl'].str.replace('?utm_source=icomarks', '', regex=False)\n",
    "## todo\n",
    "# - platform\n",
    "# - TokenAvailForSale\n",
    "# - TokenTotSupply\n",
    "# - AcceptedCurr ?\n",
    "print('\\n\\n\\n----######   Merging with category dataset   ######----')\n",
    "cat_list_rows=cat_list.shape[0]\n",
    "final_df=cat_list.copy().merge(format_df, on='url', how='left')\n",
    "display(final_df['Status'].value_counts().to_frame())\n",
    "print(f'   - Total rows: {len(final_df)}')\n",
    "if final_df.shape[0] != cat_list_rows:\n",
    "    print('########## \"final_df\" expected rows do not match')\n",
    "# save file\n",
    "save_path=os.path.join(RESULTS_FOLDER, '01d_ICOmarks_ico_list_scraped_formatted.csv')\n",
    "save_path_pkl=os.path.join(CHECKPOINT_FOLDER, 'formatted_df.pkl')\n",
    "final_df.to_csv(save_path, index=False, sep=';')\n",
    "final_df.to_pickle(save_path_pkl, protocol=-1)\n",
    "print(f'\\n   - Data saved in {save_path}')\n",
    "print(f'   - Pickle saved in {save_path_pkl}')\n",
    "# save stats\n",
    "save_path=os.path.join(RESULTS_FOLDER, '01d_ICOmarks_ico_list_scraped_formatted_Stats.csv')\n",
    "print(f'   - Stats saved in {save_path}')\n",
    "summary_stats(final_df).to_csv(save_path, index=False, sep=';')\n",
    "\n",
    "print('\\n\\nTotal elapsed time:', str(datetime.timedelta(seconds=round(timer()-start))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d71bc9",
   "metadata": {},
   "source": [
    "## Extract Market Price and Social Users list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af40328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_path=os.path.join(CHECKPOINT_FOLDER, 'scrape_df_raw.pkl')\n",
    "scrape_df=joblib.load(pkl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "69e2c02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Downloaded Market Price: 724  Total rows: 335746\n",
      "   - Data saved in .\\Results\\01e_ICOmarks_market_price_series.csv\n",
      "   - Pickle saved in .\\Checkpoints\\ICOmarks_market_price_series.pkl\n",
      "\n",
      "- Downloaded Social Users: 6398  Total rows: 7276469\n",
      "   - Data saved in .\\Results\\01e_ICOmarks_social_users_series.csv\n",
      "   - Pickle saved in .\\Checkpoints\\ICOmarks_social_users_series.pkl\n"
     ]
    }
   ],
   "source": [
    "price_df=pd.DataFrame()\n",
    "social_df=pd.DataFrame()\n",
    "for index, row in scrape_df.iterrows():\n",
    "    \n",
    "    # price\n",
    "    if row['MarketPriceSeriesStatus'] == 'DOWNLOADED':\n",
    "        tt=row['MarketPriceSeries'].copy()\n",
    "        tt.insert(0, 'url', row['url'])\n",
    "        price_df=pd.concat([price_df, tt])\n",
    "    \n",
    "    # social users\n",
    "    if row['SocialSeriesStatus'] == 'DOWNLOADED':\n",
    "        tt=pd.DataFrame()\n",
    "        for social_name, social_data in row['SocialBlock'][0]['timeseries'].items():\n",
    "            df=social_data.copy()\n",
    "            df.insert(0, 'Social', social_name)\n",
    "            df.insert(0, 'url', row['url'])\n",
    "            tt=pd.concat([tt, df])\n",
    "        social_df=pd.concat([social_df, tt])\n",
    "     \n",
    "tot_url=price_df['url'].nunique()\n",
    "print(f'- Downloaded Market Price: {tot_url}  Total rows: {len(price_df)}')\n",
    "save_path=os.path.join(RESULTS_FOLDER, '01e_ICOmarks_market_price_series.csv')\n",
    "save_path_pkl=os.path.join(CHECKPOINT_FOLDER, 'ICOmarks_market_price_series.pkl')\n",
    "price_df.to_csv(save_path, index=False, sep=';')\n",
    "price_df.to_pickle(save_path_pkl, protocol=-1)\n",
    "print(f'   - Data saved in {save_path}')\n",
    "print(f'   - Pickle saved in {save_path_pkl}')\n",
    "\n",
    "\n",
    "tot_url=social_df['url'].nunique()\n",
    "print(f'\\n- Downloaded Social Users: {tot_url}  Total rows: {len(social_df)}')\n",
    "save_path=os.path.join(RESULTS_FOLDER, '01e_ICOmarks_social_users_series.csv')\n",
    "save_path_pkl=os.path.join(CHECKPOINT_FOLDER, 'ICOmarks_social_users_series.pkl')\n",
    "# social_df.to_csv(save_path, index=False, sep=';')\n",
    "social_df.to_pickle(save_path_pkl, protocol=-1)\n",
    "print(f'   - Data saved in {save_path}')\n",
    "print(f'   - Pickle saved in {save_path_pkl}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "025950ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ICOs)",
   "language": "python",
   "name": "ico"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
